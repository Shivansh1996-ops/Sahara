{
  "best_metric": 1.0758564472198486,
  "best_model_checkpoint": "/kaggle/working/mental_health/checkpoint-2770",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 2770,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018050541516245487,
      "grad_norm": 0.4799504578113556,
      "learning_rate": 9.981949458483755e-05,
      "loss": 2.4293,
      "step": 5
    },
    {
      "epoch": 0.036101083032490974,
      "grad_norm": 0.5720686912536621,
      "learning_rate": 9.96389891696751e-05,
      "loss": 2.4151,
      "step": 10
    },
    {
      "epoch": 0.05415162454873646,
      "grad_norm": 0.600528359413147,
      "learning_rate": 9.945848375451264e-05,
      "loss": 2.3955,
      "step": 15
    },
    {
      "epoch": 0.07220216606498195,
      "grad_norm": 0.6155421733856201,
      "learning_rate": 9.927797833935018e-05,
      "loss": 2.3388,
      "step": 20
    },
    {
      "epoch": 0.09025270758122744,
      "grad_norm": 0.7155922651290894,
      "learning_rate": 9.909747292418773e-05,
      "loss": 2.2685,
      "step": 25
    },
    {
      "epoch": 0.10830324909747292,
      "grad_norm": 0.7824532389640808,
      "learning_rate": 9.891696750902527e-05,
      "loss": 2.2683,
      "step": 30
    },
    {
      "epoch": 0.1263537906137184,
      "grad_norm": 0.8527787923812866,
      "learning_rate": 9.873646209386282e-05,
      "loss": 2.2235,
      "step": 35
    },
    {
      "epoch": 0.1444043321299639,
      "grad_norm": 0.7521323561668396,
      "learning_rate": 9.855595667870036e-05,
      "loss": 2.113,
      "step": 40
    },
    {
      "epoch": 0.1624548736462094,
      "grad_norm": 0.8364397287368774,
      "learning_rate": 9.837545126353791e-05,
      "loss": 2.1006,
      "step": 45
    },
    {
      "epoch": 0.18050541516245489,
      "grad_norm": 0.8478836417198181,
      "learning_rate": 9.819494584837545e-05,
      "loss": 2.0306,
      "step": 50
    },
    {
      "epoch": 0.19855595667870035,
      "grad_norm": 0.8991787433624268,
      "learning_rate": 9.801444043321301e-05,
      "loss": 2.0184,
      "step": 55
    },
    {
      "epoch": 0.21660649819494585,
      "grad_norm": 1.0291025638580322,
      "learning_rate": 9.783393501805054e-05,
      "loss": 1.9736,
      "step": 60
    },
    {
      "epoch": 0.23465703971119134,
      "grad_norm": 0.8951537013053894,
      "learning_rate": 9.765342960288809e-05,
      "loss": 1.956,
      "step": 65
    },
    {
      "epoch": 0.2527075812274368,
      "grad_norm": 1.081547498703003,
      "learning_rate": 9.747292418772563e-05,
      "loss": 1.8692,
      "step": 70
    },
    {
      "epoch": 0.27075812274368233,
      "grad_norm": 1.0143725872039795,
      "learning_rate": 9.729241877256318e-05,
      "loss": 1.84,
      "step": 75
    },
    {
      "epoch": 0.2888086642599278,
      "grad_norm": 0.983823299407959,
      "learning_rate": 9.711191335740072e-05,
      "loss": 1.8685,
      "step": 80
    },
    {
      "epoch": 0.30685920577617326,
      "grad_norm": 0.9855466485023499,
      "learning_rate": 9.693140794223827e-05,
      "loss": 1.8079,
      "step": 85
    },
    {
      "epoch": 0.3249097472924188,
      "grad_norm": 1.0154377222061157,
      "learning_rate": 9.675090252707581e-05,
      "loss": 1.7905,
      "step": 90
    },
    {
      "epoch": 0.34296028880866425,
      "grad_norm": 1.0024654865264893,
      "learning_rate": 9.657039711191336e-05,
      "loss": 1.7731,
      "step": 95
    },
    {
      "epoch": 0.36101083032490977,
      "grad_norm": 1.0159112215042114,
      "learning_rate": 9.638989169675092e-05,
      "loss": 1.7456,
      "step": 100
    },
    {
      "epoch": 0.37906137184115524,
      "grad_norm": 0.9728909134864807,
      "learning_rate": 9.620938628158845e-05,
      "loss": 1.755,
      "step": 105
    },
    {
      "epoch": 0.3971119133574007,
      "grad_norm": 0.9422208666801453,
      "learning_rate": 9.6028880866426e-05,
      "loss": 1.7449,
      "step": 110
    },
    {
      "epoch": 0.4151624548736462,
      "grad_norm": 1.0507164001464844,
      "learning_rate": 9.584837545126354e-05,
      "loss": 1.682,
      "step": 115
    },
    {
      "epoch": 0.4332129963898917,
      "grad_norm": 1.0674508810043335,
      "learning_rate": 9.56678700361011e-05,
      "loss": 1.6886,
      "step": 120
    },
    {
      "epoch": 0.45126353790613716,
      "grad_norm": 1.040513277053833,
      "learning_rate": 9.548736462093863e-05,
      "loss": 1.6462,
      "step": 125
    },
    {
      "epoch": 0.4693140794223827,
      "grad_norm": 1.129828929901123,
      "learning_rate": 9.530685920577617e-05,
      "loss": 1.6613,
      "step": 130
    },
    {
      "epoch": 0.48736462093862815,
      "grad_norm": 1.0892366170883179,
      "learning_rate": 9.512635379061372e-05,
      "loss": 1.6642,
      "step": 135
    },
    {
      "epoch": 0.5054151624548736,
      "grad_norm": 1.0254496335983276,
      "learning_rate": 9.494584837545126e-05,
      "loss": 1.6489,
      "step": 140
    },
    {
      "epoch": 0.5234657039711191,
      "grad_norm": 1.0774040222167969,
      "learning_rate": 9.476534296028882e-05,
      "loss": 1.6281,
      "step": 145
    },
    {
      "epoch": 0.5415162454873647,
      "grad_norm": 1.1497924327850342,
      "learning_rate": 9.458483754512635e-05,
      "loss": 1.6281,
      "step": 150
    },
    {
      "epoch": 0.5595667870036101,
      "grad_norm": 1.10801100730896,
      "learning_rate": 9.44043321299639e-05,
      "loss": 1.61,
      "step": 155
    },
    {
      "epoch": 0.5776173285198556,
      "grad_norm": 1.1281651258468628,
      "learning_rate": 9.422382671480144e-05,
      "loss": 1.567,
      "step": 160
    },
    {
      "epoch": 0.5956678700361011,
      "grad_norm": 1.0744428634643555,
      "learning_rate": 9.4043321299639e-05,
      "loss": 1.5999,
      "step": 165
    },
    {
      "epoch": 0.6137184115523465,
      "grad_norm": 1.0639756917953491,
      "learning_rate": 9.386281588447655e-05,
      "loss": 1.6369,
      "step": 170
    },
    {
      "epoch": 0.631768953068592,
      "grad_norm": 1.0639493465423584,
      "learning_rate": 9.368231046931408e-05,
      "loss": 1.5559,
      "step": 175
    },
    {
      "epoch": 0.6498194945848376,
      "grad_norm": 1.0902808904647827,
      "learning_rate": 9.350180505415162e-05,
      "loss": 1.5737,
      "step": 180
    },
    {
      "epoch": 0.6678700361010831,
      "grad_norm": 1.1517874002456665,
      "learning_rate": 9.332129963898918e-05,
      "loss": 1.5604,
      "step": 185
    },
    {
      "epoch": 0.6859205776173285,
      "grad_norm": 1.13418447971344,
      "learning_rate": 9.314079422382673e-05,
      "loss": 1.5612,
      "step": 190
    },
    {
      "epoch": 0.703971119133574,
      "grad_norm": 1.1709139347076416,
      "learning_rate": 9.296028880866426e-05,
      "loss": 1.5816,
      "step": 195
    },
    {
      "epoch": 0.7220216606498195,
      "grad_norm": 1.0647858381271362,
      "learning_rate": 9.27797833935018e-05,
      "loss": 1.5269,
      "step": 200
    },
    {
      "epoch": 0.740072202166065,
      "grad_norm": 1.184875249862671,
      "learning_rate": 9.259927797833935e-05,
      "loss": 1.5869,
      "step": 205
    },
    {
      "epoch": 0.7581227436823105,
      "grad_norm": 1.1216622591018677,
      "learning_rate": 9.24187725631769e-05,
      "loss": 1.5287,
      "step": 210
    },
    {
      "epoch": 0.776173285198556,
      "grad_norm": 1.1824272871017456,
      "learning_rate": 9.223826714801445e-05,
      "loss": 1.5278,
      "step": 215
    },
    {
      "epoch": 0.7942238267148014,
      "grad_norm": 1.1288498640060425,
      "learning_rate": 9.205776173285198e-05,
      "loss": 1.5941,
      "step": 220
    },
    {
      "epoch": 0.8122743682310469,
      "grad_norm": 1.2373745441436768,
      "learning_rate": 9.187725631768953e-05,
      "loss": 1.5395,
      "step": 225
    },
    {
      "epoch": 0.8303249097472925,
      "grad_norm": 1.2021428346633911,
      "learning_rate": 9.169675090252709e-05,
      "loss": 1.5841,
      "step": 230
    },
    {
      "epoch": 0.8483754512635379,
      "grad_norm": 1.1418931484222412,
      "learning_rate": 9.151624548736463e-05,
      "loss": 1.55,
      "step": 235
    },
    {
      "epoch": 0.8664259927797834,
      "grad_norm": 1.0973631143569946,
      "learning_rate": 9.133574007220218e-05,
      "loss": 1.5184,
      "step": 240
    },
    {
      "epoch": 0.8844765342960289,
      "grad_norm": 1.126129388809204,
      "learning_rate": 9.115523465703971e-05,
      "loss": 1.5208,
      "step": 245
    },
    {
      "epoch": 0.9025270758122743,
      "grad_norm": 1.1561564207077026,
      "learning_rate": 9.097472924187727e-05,
      "loss": 1.543,
      "step": 250
    },
    {
      "epoch": 0.9205776173285198,
      "grad_norm": 1.2025471925735474,
      "learning_rate": 9.079422382671481e-05,
      "loss": 1.5145,
      "step": 255
    },
    {
      "epoch": 0.9386281588447654,
      "grad_norm": 1.2269293069839478,
      "learning_rate": 9.061371841155236e-05,
      "loss": 1.5172,
      "step": 260
    },
    {
      "epoch": 0.9566787003610109,
      "grad_norm": 1.1495863199234009,
      "learning_rate": 9.043321299638989e-05,
      "loss": 1.5198,
      "step": 265
    },
    {
      "epoch": 0.9747292418772563,
      "grad_norm": 1.1821647882461548,
      "learning_rate": 9.025270758122743e-05,
      "loss": 1.5505,
      "step": 270
    },
    {
      "epoch": 0.9927797833935018,
      "grad_norm": 1.255910873413086,
      "learning_rate": 9.007220216606499e-05,
      "loss": 1.4618,
      "step": 275
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.3121141195297241,
      "eval_runtime": 109.9097,
      "eval_samples_per_second": 8.061,
      "eval_steps_per_second": 1.01,
      "step": 277
    },
    {
      "epoch": 1.0108303249097472,
      "grad_norm": 1.3525499105453491,
      "learning_rate": 8.989169675090254e-05,
      "loss": 1.4758,
      "step": 280
    },
    {
      "epoch": 1.0288808664259927,
      "grad_norm": 1.1244258880615234,
      "learning_rate": 8.971119133574008e-05,
      "loss": 1.5299,
      "step": 285
    },
    {
      "epoch": 1.0469314079422383,
      "grad_norm": 1.1747517585754395,
      "learning_rate": 8.953068592057761e-05,
      "loss": 1.4865,
      "step": 290
    },
    {
      "epoch": 1.0649819494584838,
      "grad_norm": 1.1531447172164917,
      "learning_rate": 8.935018050541517e-05,
      "loss": 1.4686,
      "step": 295
    },
    {
      "epoch": 1.0830324909747293,
      "grad_norm": 1.239112377166748,
      "learning_rate": 8.916967509025272e-05,
      "loss": 1.5009,
      "step": 300
    },
    {
      "epoch": 1.1010830324909748,
      "grad_norm": 1.179348111152649,
      "learning_rate": 8.898916967509026e-05,
      "loss": 1.4724,
      "step": 305
    },
    {
      "epoch": 1.1191335740072201,
      "grad_norm": 1.1686723232269287,
      "learning_rate": 8.88086642599278e-05,
      "loss": 1.4457,
      "step": 310
    },
    {
      "epoch": 1.1371841155234657,
      "grad_norm": 1.2309967279434204,
      "learning_rate": 8.862815884476535e-05,
      "loss": 1.5131,
      "step": 315
    },
    {
      "epoch": 1.1552346570397112,
      "grad_norm": 1.165096640586853,
      "learning_rate": 8.84476534296029e-05,
      "loss": 1.4832,
      "step": 320
    },
    {
      "epoch": 1.1732851985559567,
      "grad_norm": 1.3095943927764893,
      "learning_rate": 8.826714801444044e-05,
      "loss": 1.4919,
      "step": 325
    },
    {
      "epoch": 1.1913357400722022,
      "grad_norm": 1.384272813796997,
      "learning_rate": 8.808664259927798e-05,
      "loss": 1.4729,
      "step": 330
    },
    {
      "epoch": 1.2093862815884477,
      "grad_norm": 1.2505022287368774,
      "learning_rate": 8.790613718411552e-05,
      "loss": 1.5093,
      "step": 335
    },
    {
      "epoch": 1.2274368231046933,
      "grad_norm": 1.2328475713729858,
      "learning_rate": 8.772563176895307e-05,
      "loss": 1.464,
      "step": 340
    },
    {
      "epoch": 1.2454873646209386,
      "grad_norm": 1.2196451425552368,
      "learning_rate": 8.754512635379062e-05,
      "loss": 1.5016,
      "step": 345
    },
    {
      "epoch": 1.263537906137184,
      "grad_norm": 1.313781976699829,
      "learning_rate": 8.736462093862816e-05,
      "loss": 1.4957,
      "step": 350
    },
    {
      "epoch": 1.2815884476534296,
      "grad_norm": 1.177088737487793,
      "learning_rate": 8.718411552346571e-05,
      "loss": 1.4231,
      "step": 355
    },
    {
      "epoch": 1.2996389891696751,
      "grad_norm": 1.4328765869140625,
      "learning_rate": 8.700361010830325e-05,
      "loss": 1.4619,
      "step": 360
    },
    {
      "epoch": 1.3176895306859207,
      "grad_norm": 1.516463279724121,
      "learning_rate": 8.68231046931408e-05,
      "loss": 1.4819,
      "step": 365
    },
    {
      "epoch": 1.335740072202166,
      "grad_norm": 1.393956184387207,
      "learning_rate": 8.664259927797834e-05,
      "loss": 1.4451,
      "step": 370
    },
    {
      "epoch": 1.3537906137184115,
      "grad_norm": 1.2668544054031372,
      "learning_rate": 8.646209386281589e-05,
      "loss": 1.4727,
      "step": 375
    },
    {
      "epoch": 1.371841155234657,
      "grad_norm": 1.33253812789917,
      "learning_rate": 8.628158844765343e-05,
      "loss": 1.4336,
      "step": 380
    },
    {
      "epoch": 1.3898916967509025,
      "grad_norm": 1.2700272798538208,
      "learning_rate": 8.610108303249098e-05,
      "loss": 1.4607,
      "step": 385
    },
    {
      "epoch": 1.407942238267148,
      "grad_norm": 1.353482723236084,
      "learning_rate": 8.592057761732852e-05,
      "loss": 1.467,
      "step": 390
    },
    {
      "epoch": 1.4259927797833936,
      "grad_norm": 1.3952836990356445,
      "learning_rate": 8.574007220216607e-05,
      "loss": 1.4647,
      "step": 395
    },
    {
      "epoch": 1.444043321299639,
      "grad_norm": 1.3371623754501343,
      "learning_rate": 8.555956678700361e-05,
      "loss": 1.4009,
      "step": 400
    },
    {
      "epoch": 1.4620938628158844,
      "grad_norm": 1.3734114170074463,
      "learning_rate": 8.537906137184116e-05,
      "loss": 1.424,
      "step": 405
    },
    {
      "epoch": 1.48014440433213,
      "grad_norm": 1.5049571990966797,
      "learning_rate": 8.51985559566787e-05,
      "loss": 1.4763,
      "step": 410
    },
    {
      "epoch": 1.4981949458483754,
      "grad_norm": 1.384716272354126,
      "learning_rate": 8.501805054151625e-05,
      "loss": 1.4592,
      "step": 415
    },
    {
      "epoch": 1.516245487364621,
      "grad_norm": 1.3281201124191284,
      "learning_rate": 8.48375451263538e-05,
      "loss": 1.4351,
      "step": 420
    },
    {
      "epoch": 1.5342960288808665,
      "grad_norm": 1.3741766214370728,
      "learning_rate": 8.465703971119134e-05,
      "loss": 1.4363,
      "step": 425
    },
    {
      "epoch": 1.5523465703971118,
      "grad_norm": 1.4475793838500977,
      "learning_rate": 8.447653429602888e-05,
      "loss": 1.4276,
      "step": 430
    },
    {
      "epoch": 1.5703971119133575,
      "grad_norm": 1.5578117370605469,
      "learning_rate": 8.429602888086643e-05,
      "loss": 1.4482,
      "step": 435
    },
    {
      "epoch": 1.5884476534296028,
      "grad_norm": 1.285927176475525,
      "learning_rate": 8.411552346570397e-05,
      "loss": 1.4758,
      "step": 440
    },
    {
      "epoch": 1.6064981949458483,
      "grad_norm": 1.397343635559082,
      "learning_rate": 8.393501805054152e-05,
      "loss": 1.4246,
      "step": 445
    },
    {
      "epoch": 1.6245487364620939,
      "grad_norm": 1.3319860696792603,
      "learning_rate": 8.375451263537906e-05,
      "loss": 1.4354,
      "step": 450
    },
    {
      "epoch": 1.6425992779783394,
      "grad_norm": 1.2913503646850586,
      "learning_rate": 8.357400722021661e-05,
      "loss": 1.4685,
      "step": 455
    },
    {
      "epoch": 1.660649819494585,
      "grad_norm": 1.3372786045074463,
      "learning_rate": 8.339350180505415e-05,
      "loss": 1.4576,
      "step": 460
    },
    {
      "epoch": 1.6787003610108302,
      "grad_norm": 1.3097747564315796,
      "learning_rate": 8.32129963898917e-05,
      "loss": 1.4202,
      "step": 465
    },
    {
      "epoch": 1.696750902527076,
      "grad_norm": 1.3357502222061157,
      "learning_rate": 8.303249097472924e-05,
      "loss": 1.3967,
      "step": 470
    },
    {
      "epoch": 1.7148014440433212,
      "grad_norm": 1.4531264305114746,
      "learning_rate": 8.285198555956679e-05,
      "loss": 1.3999,
      "step": 475
    },
    {
      "epoch": 1.7328519855595668,
      "grad_norm": 1.5493556261062622,
      "learning_rate": 8.267148014440433e-05,
      "loss": 1.3744,
      "step": 480
    },
    {
      "epoch": 1.7509025270758123,
      "grad_norm": 1.586108684539795,
      "learning_rate": 8.249097472924188e-05,
      "loss": 1.4305,
      "step": 485
    },
    {
      "epoch": 1.7689530685920578,
      "grad_norm": 1.396033525466919,
      "learning_rate": 8.231046931407944e-05,
      "loss": 1.4237,
      "step": 490
    },
    {
      "epoch": 1.7870036101083033,
      "grad_norm": 1.3931989669799805,
      "learning_rate": 8.212996389891697e-05,
      "loss": 1.4286,
      "step": 495
    },
    {
      "epoch": 1.8050541516245486,
      "grad_norm": 1.484681487083435,
      "learning_rate": 8.194945848375451e-05,
      "loss": 1.4517,
      "step": 500
    },
    {
      "epoch": 1.8231046931407944,
      "grad_norm": 1.378014087677002,
      "learning_rate": 8.176895306859206e-05,
      "loss": 1.4264,
      "step": 505
    },
    {
      "epoch": 1.8411552346570397,
      "grad_norm": 1.4125043153762817,
      "learning_rate": 8.158844765342962e-05,
      "loss": 1.4513,
      "step": 510
    },
    {
      "epoch": 1.8592057761732852,
      "grad_norm": 1.558781385421753,
      "learning_rate": 8.140794223826715e-05,
      "loss": 1.376,
      "step": 515
    },
    {
      "epoch": 1.8772563176895307,
      "grad_norm": 1.574154257774353,
      "learning_rate": 8.122743682310469e-05,
      "loss": 1.4506,
      "step": 520
    },
    {
      "epoch": 1.895306859205776,
      "grad_norm": 1.4001084566116333,
      "learning_rate": 8.104693140794224e-05,
      "loss": 1.4163,
      "step": 525
    },
    {
      "epoch": 1.9133574007220218,
      "grad_norm": 1.4130887985229492,
      "learning_rate": 8.086642599277978e-05,
      "loss": 1.4606,
      "step": 530
    },
    {
      "epoch": 1.931407942238267,
      "grad_norm": 1.403996467590332,
      "learning_rate": 8.068592057761734e-05,
      "loss": 1.3906,
      "step": 535
    },
    {
      "epoch": 1.9494584837545126,
      "grad_norm": 1.601577877998352,
      "learning_rate": 8.050541516245487e-05,
      "loss": 1.3906,
      "step": 540
    },
    {
      "epoch": 1.967509025270758,
      "grad_norm": 1.3686877489089966,
      "learning_rate": 8.032490974729242e-05,
      "loss": 1.3772,
      "step": 545
    },
    {
      "epoch": 1.9855595667870036,
      "grad_norm": 1.4003506898880005,
      "learning_rate": 8.014440433212996e-05,
      "loss": 1.451,
      "step": 550
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.213046908378601,
      "eval_runtime": 110.0556,
      "eval_samples_per_second": 8.05,
      "eval_steps_per_second": 1.009,
      "step": 554
    },
    {
      "epoch": 2.003610108303249,
      "grad_norm": 1.5059847831726074,
      "learning_rate": 7.996389891696752e-05,
      "loss": 1.4138,
      "step": 555
    },
    {
      "epoch": 2.0216606498194944,
      "grad_norm": 1.5349102020263672,
      "learning_rate": 7.978339350180507e-05,
      "loss": 1.4001,
      "step": 560
    },
    {
      "epoch": 2.03971119133574,
      "grad_norm": 1.341372013092041,
      "learning_rate": 7.96028880866426e-05,
      "loss": 1.3901,
      "step": 565
    },
    {
      "epoch": 2.0577617328519855,
      "grad_norm": 1.53367280960083,
      "learning_rate": 7.942238267148014e-05,
      "loss": 1.352,
      "step": 570
    },
    {
      "epoch": 2.0758122743682312,
      "grad_norm": 1.536698341369629,
      "learning_rate": 7.92418772563177e-05,
      "loss": 1.3914,
      "step": 575
    },
    {
      "epoch": 2.0938628158844765,
      "grad_norm": 1.4836690425872803,
      "learning_rate": 7.906137184115525e-05,
      "loss": 1.439,
      "step": 580
    },
    {
      "epoch": 2.111913357400722,
      "grad_norm": 1.4735045433044434,
      "learning_rate": 7.888086642599278e-05,
      "loss": 1.4446,
      "step": 585
    },
    {
      "epoch": 2.1299638989169676,
      "grad_norm": 1.4337217807769775,
      "learning_rate": 7.870036101083032e-05,
      "loss": 1.4212,
      "step": 590
    },
    {
      "epoch": 2.148014440433213,
      "grad_norm": 1.5845237970352173,
      "learning_rate": 7.851985559566787e-05,
      "loss": 1.3699,
      "step": 595
    },
    {
      "epoch": 2.1660649819494586,
      "grad_norm": 1.5258067846298218,
      "learning_rate": 7.833935018050543e-05,
      "loss": 1.3792,
      "step": 600
    },
    {
      "epoch": 2.184115523465704,
      "grad_norm": 1.6057119369506836,
      "learning_rate": 7.815884476534297e-05,
      "loss": 1.3521,
      "step": 605
    },
    {
      "epoch": 2.2021660649819497,
      "grad_norm": 1.5607038736343384,
      "learning_rate": 7.79783393501805e-05,
      "loss": 1.425,
      "step": 610
    },
    {
      "epoch": 2.220216606498195,
      "grad_norm": 1.5239230394363403,
      "learning_rate": 7.779783393501805e-05,
      "loss": 1.3774,
      "step": 615
    },
    {
      "epoch": 2.2382671480144403,
      "grad_norm": 1.469407558441162,
      "learning_rate": 7.76173285198556e-05,
      "loss": 1.3607,
      "step": 620
    },
    {
      "epoch": 2.256317689530686,
      "grad_norm": 1.5201486349105835,
      "learning_rate": 7.743682310469315e-05,
      "loss": 1.3524,
      "step": 625
    },
    {
      "epoch": 2.2743682310469313,
      "grad_norm": 1.4865314960479736,
      "learning_rate": 7.72563176895307e-05,
      "loss": 1.3682,
      "step": 630
    },
    {
      "epoch": 2.292418772563177,
      "grad_norm": 1.5658740997314453,
      "learning_rate": 7.707581227436823e-05,
      "loss": 1.3795,
      "step": 635
    },
    {
      "epoch": 2.3104693140794224,
      "grad_norm": 1.4949532747268677,
      "learning_rate": 7.689530685920579e-05,
      "loss": 1.3445,
      "step": 640
    },
    {
      "epoch": 2.328519855595668,
      "grad_norm": 1.5115091800689697,
      "learning_rate": 7.671480144404333e-05,
      "loss": 1.3133,
      "step": 645
    },
    {
      "epoch": 2.3465703971119134,
      "grad_norm": 1.5614029169082642,
      "learning_rate": 7.653429602888087e-05,
      "loss": 1.3872,
      "step": 650
    },
    {
      "epoch": 2.3646209386281587,
      "grad_norm": 1.6752607822418213,
      "learning_rate": 7.63537906137184e-05,
      "loss": 1.3981,
      "step": 655
    },
    {
      "epoch": 2.3826714801444044,
      "grad_norm": 1.5977630615234375,
      "learning_rate": 7.617328519855595e-05,
      "loss": 1.3914,
      "step": 660
    },
    {
      "epoch": 2.4007220216606497,
      "grad_norm": 1.5923290252685547,
      "learning_rate": 7.599277978339351e-05,
      "loss": 1.3877,
      "step": 665
    },
    {
      "epoch": 2.4187725631768955,
      "grad_norm": 1.5701236724853516,
      "learning_rate": 7.581227436823105e-05,
      "loss": 1.4209,
      "step": 670
    },
    {
      "epoch": 2.436823104693141,
      "grad_norm": 1.5541062355041504,
      "learning_rate": 7.56317689530686e-05,
      "loss": 1.3752,
      "step": 675
    },
    {
      "epoch": 2.4548736462093865,
      "grad_norm": 1.5323987007141113,
      "learning_rate": 7.545126353790613e-05,
      "loss": 1.4006,
      "step": 680
    },
    {
      "epoch": 2.472924187725632,
      "grad_norm": 1.593470811843872,
      "learning_rate": 7.527075812274369e-05,
      "loss": 1.4109,
      "step": 685
    },
    {
      "epoch": 2.490974729241877,
      "grad_norm": 1.4547693729400635,
      "learning_rate": 7.509025270758123e-05,
      "loss": 1.3818,
      "step": 690
    },
    {
      "epoch": 2.509025270758123,
      "grad_norm": 1.4605203866958618,
      "learning_rate": 7.490974729241878e-05,
      "loss": 1.3646,
      "step": 695
    },
    {
      "epoch": 2.527075812274368,
      "grad_norm": 1.5139235258102417,
      "learning_rate": 7.472924187725631e-05,
      "loss": 1.3986,
      "step": 700
    },
    {
      "epoch": 2.5451263537906135,
      "grad_norm": 1.544476866722107,
      "learning_rate": 7.454873646209387e-05,
      "loss": 1.348,
      "step": 705
    },
    {
      "epoch": 2.563176895306859,
      "grad_norm": 1.608977198600769,
      "learning_rate": 7.436823104693141e-05,
      "loss": 1.3811,
      "step": 710
    },
    {
      "epoch": 2.581227436823105,
      "grad_norm": 1.7039889097213745,
      "learning_rate": 7.418772563176896e-05,
      "loss": 1.3841,
      "step": 715
    },
    {
      "epoch": 2.5992779783393503,
      "grad_norm": 1.630855679512024,
      "learning_rate": 7.40072202166065e-05,
      "loss": 1.3406,
      "step": 720
    },
    {
      "epoch": 2.6173285198555956,
      "grad_norm": 1.452458381652832,
      "learning_rate": 7.382671480144405e-05,
      "loss": 1.4206,
      "step": 725
    },
    {
      "epoch": 2.6353790613718413,
      "grad_norm": 1.5962086915969849,
      "learning_rate": 7.36462093862816e-05,
      "loss": 1.3257,
      "step": 730
    },
    {
      "epoch": 2.6534296028880866,
      "grad_norm": 1.6211719512939453,
      "learning_rate": 7.346570397111914e-05,
      "loss": 1.3566,
      "step": 735
    },
    {
      "epoch": 2.671480144404332,
      "grad_norm": 1.5686389207839966,
      "learning_rate": 7.328519855595668e-05,
      "loss": 1.3902,
      "step": 740
    },
    {
      "epoch": 2.6895306859205776,
      "grad_norm": 1.4982558488845825,
      "learning_rate": 7.310469314079423e-05,
      "loss": 1.3501,
      "step": 745
    },
    {
      "epoch": 2.707581227436823,
      "grad_norm": 1.4581562280654907,
      "learning_rate": 7.292418772563177e-05,
      "loss": 1.4021,
      "step": 750
    },
    {
      "epoch": 2.7256317689530687,
      "grad_norm": 1.5516725778579712,
      "learning_rate": 7.274368231046932e-05,
      "loss": 1.356,
      "step": 755
    },
    {
      "epoch": 2.743682310469314,
      "grad_norm": 1.5671485662460327,
      "learning_rate": 7.256317689530686e-05,
      "loss": 1.3458,
      "step": 760
    },
    {
      "epoch": 2.7617328519855597,
      "grad_norm": 1.530051350593567,
      "learning_rate": 7.238267148014441e-05,
      "loss": 1.3562,
      "step": 765
    },
    {
      "epoch": 2.779783393501805,
      "grad_norm": 1.5621166229248047,
      "learning_rate": 7.220216606498195e-05,
      "loss": 1.3864,
      "step": 770
    },
    {
      "epoch": 2.7978339350180503,
      "grad_norm": 1.6763275861740112,
      "learning_rate": 7.20216606498195e-05,
      "loss": 1.3673,
      "step": 775
    },
    {
      "epoch": 2.815884476534296,
      "grad_norm": 1.5553861856460571,
      "learning_rate": 7.184115523465704e-05,
      "loss": 1.3994,
      "step": 780
    },
    {
      "epoch": 2.8339350180505414,
      "grad_norm": 1.6232868432998657,
      "learning_rate": 7.166064981949459e-05,
      "loss": 1.3753,
      "step": 785
    },
    {
      "epoch": 2.851985559566787,
      "grad_norm": 1.5935142040252686,
      "learning_rate": 7.148014440433213e-05,
      "loss": 1.3811,
      "step": 790
    },
    {
      "epoch": 2.8700361010830324,
      "grad_norm": 1.5183970928192139,
      "learning_rate": 7.129963898916968e-05,
      "loss": 1.4015,
      "step": 795
    },
    {
      "epoch": 2.888086642599278,
      "grad_norm": 1.595733880996704,
      "learning_rate": 7.111913357400722e-05,
      "loss": 1.3633,
      "step": 800
    },
    {
      "epoch": 2.9061371841155235,
      "grad_norm": 1.4199614524841309,
      "learning_rate": 7.093862815884477e-05,
      "loss": 1.3326,
      "step": 805
    },
    {
      "epoch": 2.9241877256317688,
      "grad_norm": 1.779373288154602,
      "learning_rate": 7.075812274368231e-05,
      "loss": 1.3813,
      "step": 810
    },
    {
      "epoch": 2.9422382671480145,
      "grad_norm": 1.6033529043197632,
      "learning_rate": 7.057761732851986e-05,
      "loss": 1.3175,
      "step": 815
    },
    {
      "epoch": 2.96028880866426,
      "grad_norm": 1.7475247383117676,
      "learning_rate": 7.03971119133574e-05,
      "loss": 1.3508,
      "step": 820
    },
    {
      "epoch": 2.9783393501805056,
      "grad_norm": 1.5771242380142212,
      "learning_rate": 7.021660649819495e-05,
      "loss": 1.346,
      "step": 825
    },
    {
      "epoch": 2.996389891696751,
      "grad_norm": 1.600412368774414,
      "learning_rate": 7.003610108303249e-05,
      "loss": 1.3447,
      "step": 830
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.1676411628723145,
      "eval_runtime": 109.6106,
      "eval_samples_per_second": 8.083,
      "eval_steps_per_second": 1.013,
      "step": 831
    },
    {
      "epoch": 3.0144404332129966,
      "grad_norm": 1.6724027395248413,
      "learning_rate": 6.985559566787004e-05,
      "loss": 1.3115,
      "step": 835
    },
    {
      "epoch": 3.032490974729242,
      "grad_norm": 1.6937447786331177,
      "learning_rate": 6.967509025270758e-05,
      "loss": 1.3655,
      "step": 840
    },
    {
      "epoch": 3.050541516245487,
      "grad_norm": 1.5215321779251099,
      "learning_rate": 6.949458483754513e-05,
      "loss": 1.3418,
      "step": 845
    },
    {
      "epoch": 3.068592057761733,
      "grad_norm": 1.6468751430511475,
      "learning_rate": 6.931407942238267e-05,
      "loss": 1.337,
      "step": 850
    },
    {
      "epoch": 3.0866425992779782,
      "grad_norm": 1.7726160287857056,
      "learning_rate": 6.913357400722023e-05,
      "loss": 1.3544,
      "step": 855
    },
    {
      "epoch": 3.104693140794224,
      "grad_norm": 1.6506088972091675,
      "learning_rate": 6.895306859205776e-05,
      "loss": 1.2898,
      "step": 860
    },
    {
      "epoch": 3.1227436823104693,
      "grad_norm": 1.6146737337112427,
      "learning_rate": 6.877256317689531e-05,
      "loss": 1.3655,
      "step": 865
    },
    {
      "epoch": 3.140794223826715,
      "grad_norm": 1.6397955417633057,
      "learning_rate": 6.859205776173285e-05,
      "loss": 1.3077,
      "step": 870
    },
    {
      "epoch": 3.1588447653429603,
      "grad_norm": 1.559328317642212,
      "learning_rate": 6.84115523465704e-05,
      "loss": 1.3439,
      "step": 875
    },
    {
      "epoch": 3.1768953068592056,
      "grad_norm": 1.561761736869812,
      "learning_rate": 6.823104693140794e-05,
      "loss": 1.3295,
      "step": 880
    },
    {
      "epoch": 3.1949458483754514,
      "grad_norm": 1.5155513286590576,
      "learning_rate": 6.805054151624549e-05,
      "loss": 1.3188,
      "step": 885
    },
    {
      "epoch": 3.2129963898916967,
      "grad_norm": 1.632987141609192,
      "learning_rate": 6.787003610108303e-05,
      "loss": 1.3601,
      "step": 890
    },
    {
      "epoch": 3.2310469314079424,
      "grad_norm": 1.8113335371017456,
      "learning_rate": 6.768953068592058e-05,
      "loss": 1.3249,
      "step": 895
    },
    {
      "epoch": 3.2490974729241877,
      "grad_norm": 1.6504278182983398,
      "learning_rate": 6.750902527075814e-05,
      "loss": 1.3717,
      "step": 900
    },
    {
      "epoch": 3.2671480144404335,
      "grad_norm": 1.6536414623260498,
      "learning_rate": 6.732851985559567e-05,
      "loss": 1.2908,
      "step": 905
    },
    {
      "epoch": 3.2851985559566788,
      "grad_norm": 1.7374061346054077,
      "learning_rate": 6.714801444043321e-05,
      "loss": 1.2989,
      "step": 910
    },
    {
      "epoch": 3.303249097472924,
      "grad_norm": 1.7211520671844482,
      "learning_rate": 6.696750902527076e-05,
      "loss": 1.3419,
      "step": 915
    },
    {
      "epoch": 3.32129963898917,
      "grad_norm": 1.5457992553710938,
      "learning_rate": 6.678700361010832e-05,
      "loss": 1.3381,
      "step": 920
    },
    {
      "epoch": 3.339350180505415,
      "grad_norm": 1.5843377113342285,
      "learning_rate": 6.660649819494586e-05,
      "loss": 1.31,
      "step": 925
    },
    {
      "epoch": 3.357400722021661,
      "grad_norm": 1.5822328329086304,
      "learning_rate": 6.642599277978339e-05,
      "loss": 1.3426,
      "step": 930
    },
    {
      "epoch": 3.375451263537906,
      "grad_norm": 1.7187740802764893,
      "learning_rate": 6.624548736462094e-05,
      "loss": 1.3347,
      "step": 935
    },
    {
      "epoch": 3.3935018050541514,
      "grad_norm": 1.6228747367858887,
      "learning_rate": 6.606498194945848e-05,
      "loss": 1.2954,
      "step": 940
    },
    {
      "epoch": 3.411552346570397,
      "grad_norm": 1.626237154006958,
      "learning_rate": 6.588447653429604e-05,
      "loss": 1.3745,
      "step": 945
    },
    {
      "epoch": 3.4296028880866425,
      "grad_norm": 1.7105400562286377,
      "learning_rate": 6.570397111913357e-05,
      "loss": 1.3474,
      "step": 950
    },
    {
      "epoch": 3.4476534296028882,
      "grad_norm": 1.6362090110778809,
      "learning_rate": 6.552346570397112e-05,
      "loss": 1.2959,
      "step": 955
    },
    {
      "epoch": 3.4657039711191335,
      "grad_norm": 1.6419981718063354,
      "learning_rate": 6.534296028880866e-05,
      "loss": 1.343,
      "step": 960
    },
    {
      "epoch": 3.483754512635379,
      "grad_norm": 1.826389193534851,
      "learning_rate": 6.516245487364622e-05,
      "loss": 1.3351,
      "step": 965
    },
    {
      "epoch": 3.5018050541516246,
      "grad_norm": 1.7525250911712646,
      "learning_rate": 6.498194945848377e-05,
      "loss": 1.3049,
      "step": 970
    },
    {
      "epoch": 3.51985559566787,
      "grad_norm": 1.5553045272827148,
      "learning_rate": 6.48014440433213e-05,
      "loss": 1.328,
      "step": 975
    },
    {
      "epoch": 3.5379061371841156,
      "grad_norm": 1.6235002279281616,
      "learning_rate": 6.462093862815884e-05,
      "loss": 1.3233,
      "step": 980
    },
    {
      "epoch": 3.555956678700361,
      "grad_norm": 1.4878261089324951,
      "learning_rate": 6.44404332129964e-05,
      "loss": 1.3469,
      "step": 985
    },
    {
      "epoch": 3.5740072202166067,
      "grad_norm": 1.6828575134277344,
      "learning_rate": 6.425992779783394e-05,
      "loss": 1.3769,
      "step": 990
    },
    {
      "epoch": 3.592057761732852,
      "grad_norm": 1.6732796430587769,
      "learning_rate": 6.407942238267149e-05,
      "loss": 1.3588,
      "step": 995
    },
    {
      "epoch": 3.6101083032490973,
      "grad_norm": 1.712623119354248,
      "learning_rate": 6.389891696750902e-05,
      "loss": 1.2773,
      "step": 1000
    },
    {
      "epoch": 3.628158844765343,
      "grad_norm": 1.6909635066986084,
      "learning_rate": 6.371841155234657e-05,
      "loss": 1.2971,
      "step": 1005
    },
    {
      "epoch": 3.6462093862815883,
      "grad_norm": 1.663036584854126,
      "learning_rate": 6.353790613718412e-05,
      "loss": 1.3521,
      "step": 1010
    },
    {
      "epoch": 3.664259927797834,
      "grad_norm": 1.7175666093826294,
      "learning_rate": 6.335740072202167e-05,
      "loss": 1.3505,
      "step": 1015
    },
    {
      "epoch": 3.6823104693140793,
      "grad_norm": 1.737341046333313,
      "learning_rate": 6.31768953068592e-05,
      "loss": 1.3554,
      "step": 1020
    },
    {
      "epoch": 3.700361010830325,
      "grad_norm": 1.71530282497406,
      "learning_rate": 6.299638989169675e-05,
      "loss": 1.3746,
      "step": 1025
    },
    {
      "epoch": 3.7184115523465704,
      "grad_norm": 1.7381548881530762,
      "learning_rate": 6.28158844765343e-05,
      "loss": 1.3157,
      "step": 1030
    },
    {
      "epoch": 3.7364620938628157,
      "grad_norm": 1.7062212228775024,
      "learning_rate": 6.263537906137185e-05,
      "loss": 1.3612,
      "step": 1035
    },
    {
      "epoch": 3.7545126353790614,
      "grad_norm": 1.6183621883392334,
      "learning_rate": 6.24548736462094e-05,
      "loss": 1.2995,
      "step": 1040
    },
    {
      "epoch": 3.7725631768953067,
      "grad_norm": 1.6442618370056152,
      "learning_rate": 6.227436823104693e-05,
      "loss": 1.3005,
      "step": 1045
    },
    {
      "epoch": 3.7906137184115525,
      "grad_norm": 1.6789096593856812,
      "learning_rate": 6.209386281588448e-05,
      "loss": 1.3044,
      "step": 1050
    },
    {
      "epoch": 3.808664259927798,
      "grad_norm": 1.658603549003601,
      "learning_rate": 6.191335740072203e-05,
      "loss": 1.3166,
      "step": 1055
    },
    {
      "epoch": 3.8267148014440435,
      "grad_norm": 1.7218397855758667,
      "learning_rate": 6.173285198555957e-05,
      "loss": 1.2853,
      "step": 1060
    },
    {
      "epoch": 3.844765342960289,
      "grad_norm": 1.6911156177520752,
      "learning_rate": 6.155234657039712e-05,
      "loss": 1.313,
      "step": 1065
    },
    {
      "epoch": 3.862815884476534,
      "grad_norm": 1.830110788345337,
      "learning_rate": 6.137184115523465e-05,
      "loss": 1.3237,
      "step": 1070
    },
    {
      "epoch": 3.88086642599278,
      "grad_norm": 1.8437609672546387,
      "learning_rate": 6.119133574007221e-05,
      "loss": 1.3155,
      "step": 1075
    },
    {
      "epoch": 3.898916967509025,
      "grad_norm": 1.8770513534545898,
      "learning_rate": 6.1010830324909754e-05,
      "loss": 1.2787,
      "step": 1080
    },
    {
      "epoch": 3.916967509025271,
      "grad_norm": 1.7279671430587769,
      "learning_rate": 6.083032490974729e-05,
      "loss": 1.2871,
      "step": 1085
    },
    {
      "epoch": 3.935018050541516,
      "grad_norm": 1.8374234437942505,
      "learning_rate": 6.064981949458484e-05,
      "loss": 1.3239,
      "step": 1090
    },
    {
      "epoch": 3.953068592057762,
      "grad_norm": 1.9738889932632446,
      "learning_rate": 6.046931407942239e-05,
      "loss": 1.311,
      "step": 1095
    },
    {
      "epoch": 3.9711191335740073,
      "grad_norm": 1.7183046340942383,
      "learning_rate": 6.0288808664259934e-05,
      "loss": 1.3483,
      "step": 1100
    },
    {
      "epoch": 3.9891696750902526,
      "grad_norm": 1.7977635860443115,
      "learning_rate": 6.010830324909747e-05,
      "loss": 1.287,
      "step": 1105
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.1341478824615479,
      "eval_runtime": 109.3005,
      "eval_samples_per_second": 8.106,
      "eval_steps_per_second": 1.016,
      "step": 1108
    },
    {
      "epoch": 4.007220216606498,
      "grad_norm": 1.8043177127838135,
      "learning_rate": 5.992779783393502e-05,
      "loss": 1.2936,
      "step": 1110
    },
    {
      "epoch": 4.025270758122744,
      "grad_norm": 1.9102869033813477,
      "learning_rate": 5.974729241877257e-05,
      "loss": 1.2811,
      "step": 1115
    },
    {
      "epoch": 4.043321299638989,
      "grad_norm": 1.6676039695739746,
      "learning_rate": 5.9566787003610113e-05,
      "loss": 1.2716,
      "step": 1120
    },
    {
      "epoch": 4.061371841155235,
      "grad_norm": 1.8523885011672974,
      "learning_rate": 5.938628158844766e-05,
      "loss": 1.315,
      "step": 1125
    },
    {
      "epoch": 4.07942238267148,
      "grad_norm": 1.72869873046875,
      "learning_rate": 5.9205776173285197e-05,
      "loss": 1.2819,
      "step": 1130
    },
    {
      "epoch": 4.097472924187725,
      "grad_norm": 1.690872311592102,
      "learning_rate": 5.902527075812274e-05,
      "loss": 1.3005,
      "step": 1135
    },
    {
      "epoch": 4.115523465703971,
      "grad_norm": 1.6348737478256226,
      "learning_rate": 5.884476534296029e-05,
      "loss": 1.2842,
      "step": 1140
    },
    {
      "epoch": 4.133574007220217,
      "grad_norm": 1.7003108263015747,
      "learning_rate": 5.866425992779784e-05,
      "loss": 1.3112,
      "step": 1145
    },
    {
      "epoch": 4.1516245487364625,
      "grad_norm": 1.79075288772583,
      "learning_rate": 5.848375451263538e-05,
      "loss": 1.2968,
      "step": 1150
    },
    {
      "epoch": 4.169675090252707,
      "grad_norm": 1.9025566577911377,
      "learning_rate": 5.830324909747292e-05,
      "loss": 1.3123,
      "step": 1155
    },
    {
      "epoch": 4.187725631768953,
      "grad_norm": 1.7797951698303223,
      "learning_rate": 5.812274368231048e-05,
      "loss": 1.3055,
      "step": 1160
    },
    {
      "epoch": 4.205776173285199,
      "grad_norm": 1.7493016719818115,
      "learning_rate": 5.794223826714802e-05,
      "loss": 1.2707,
      "step": 1165
    },
    {
      "epoch": 4.223826714801444,
      "grad_norm": 1.986812949180603,
      "learning_rate": 5.776173285198556e-05,
      "loss": 1.3182,
      "step": 1170
    },
    {
      "epoch": 4.241877256317689,
      "grad_norm": 1.767866611480713,
      "learning_rate": 5.75812274368231e-05,
      "loss": 1.3164,
      "step": 1175
    },
    {
      "epoch": 4.259927797833935,
      "grad_norm": 1.9048644304275513,
      "learning_rate": 5.740072202166066e-05,
      "loss": 1.2985,
      "step": 1180
    },
    {
      "epoch": 4.277978339350181,
      "grad_norm": 1.816408395767212,
      "learning_rate": 5.72202166064982e-05,
      "loss": 1.3288,
      "step": 1185
    },
    {
      "epoch": 4.296028880866426,
      "grad_norm": 1.820142388343811,
      "learning_rate": 5.703971119133574e-05,
      "loss": 1.3011,
      "step": 1190
    },
    {
      "epoch": 4.3140794223826715,
      "grad_norm": 1.6470389366149902,
      "learning_rate": 5.685920577617329e-05,
      "loss": 1.3161,
      "step": 1195
    },
    {
      "epoch": 4.332129963898917,
      "grad_norm": 1.8739981651306152,
      "learning_rate": 5.6678700361010826e-05,
      "loss": 1.3407,
      "step": 1200
    },
    {
      "epoch": 4.350180505415162,
      "grad_norm": 1.7381666898727417,
      "learning_rate": 5.6498194945848384e-05,
      "loss": 1.3533,
      "step": 1205
    },
    {
      "epoch": 4.368231046931408,
      "grad_norm": 1.7991089820861816,
      "learning_rate": 5.631768953068592e-05,
      "loss": 1.2895,
      "step": 1210
    },
    {
      "epoch": 4.386281588447654,
      "grad_norm": 1.8035705089569092,
      "learning_rate": 5.613718411552347e-05,
      "loss": 1.2538,
      "step": 1215
    },
    {
      "epoch": 4.404332129963899,
      "grad_norm": 1.9350496530532837,
      "learning_rate": 5.595667870036101e-05,
      "loss": 1.3022,
      "step": 1220
    },
    {
      "epoch": 4.422382671480144,
      "grad_norm": 1.990578532218933,
      "learning_rate": 5.5776173285198564e-05,
      "loss": 1.3048,
      "step": 1225
    },
    {
      "epoch": 4.44043321299639,
      "grad_norm": 1.6907953023910522,
      "learning_rate": 5.55956678700361e-05,
      "loss": 1.3011,
      "step": 1230
    },
    {
      "epoch": 4.458483754512636,
      "grad_norm": 1.754032015800476,
      "learning_rate": 5.541516245487365e-05,
      "loss": 1.3064,
      "step": 1235
    },
    {
      "epoch": 4.4765342960288805,
      "grad_norm": 1.9556316137313843,
      "learning_rate": 5.523465703971119e-05,
      "loss": 1.2946,
      "step": 1240
    },
    {
      "epoch": 4.494584837545126,
      "grad_norm": 1.655053734779358,
      "learning_rate": 5.5054151624548744e-05,
      "loss": 1.2856,
      "step": 1245
    },
    {
      "epoch": 4.512635379061372,
      "grad_norm": 1.6652421951293945,
      "learning_rate": 5.487364620938629e-05,
      "loss": 1.3238,
      "step": 1250
    },
    {
      "epoch": 4.530685920577618,
      "grad_norm": 1.8859537839889526,
      "learning_rate": 5.469314079422383e-05,
      "loss": 1.281,
      "step": 1255
    },
    {
      "epoch": 4.548736462093863,
      "grad_norm": 1.7461766004562378,
      "learning_rate": 5.451263537906137e-05,
      "loss": 1.2912,
      "step": 1260
    },
    {
      "epoch": 4.566787003610108,
      "grad_norm": 1.8678507804870605,
      "learning_rate": 5.433212996389892e-05,
      "loss": 1.316,
      "step": 1265
    },
    {
      "epoch": 4.584837545126354,
      "grad_norm": 1.7051360607147217,
      "learning_rate": 5.415162454873647e-05,
      "loss": 1.3058,
      "step": 1270
    },
    {
      "epoch": 4.602888086642599,
      "grad_norm": 1.7397558689117432,
      "learning_rate": 5.3971119133574014e-05,
      "loss": 1.2686,
      "step": 1275
    },
    {
      "epoch": 4.620938628158845,
      "grad_norm": 1.605165958404541,
      "learning_rate": 5.379061371841155e-05,
      "loss": 1.3371,
      "step": 1280
    },
    {
      "epoch": 4.6389891696750905,
      "grad_norm": 1.883937954902649,
      "learning_rate": 5.36101083032491e-05,
      "loss": 1.2942,
      "step": 1285
    },
    {
      "epoch": 4.657039711191336,
      "grad_norm": 1.7851977348327637,
      "learning_rate": 5.342960288808665e-05,
      "loss": 1.282,
      "step": 1290
    },
    {
      "epoch": 4.675090252707581,
      "grad_norm": 1.87135648727417,
      "learning_rate": 5.324909747292419e-05,
      "loss": 1.3129,
      "step": 1295
    },
    {
      "epoch": 4.693140794223827,
      "grad_norm": 1.7924487590789795,
      "learning_rate": 5.306859205776173e-05,
      "loss": 1.3062,
      "step": 1300
    },
    {
      "epoch": 4.7111913357400725,
      "grad_norm": 1.7871837615966797,
      "learning_rate": 5.2888086642599276e-05,
      "loss": 1.3141,
      "step": 1305
    },
    {
      "epoch": 4.729241877256317,
      "grad_norm": 1.676888108253479,
      "learning_rate": 5.270758122743683e-05,
      "loss": 1.2855,
      "step": 1310
    },
    {
      "epoch": 4.747292418772563,
      "grad_norm": 1.8345824480056763,
      "learning_rate": 5.252707581227437e-05,
      "loss": 1.3177,
      "step": 1315
    },
    {
      "epoch": 4.765342960288809,
      "grad_norm": 1.8249766826629639,
      "learning_rate": 5.234657039711192e-05,
      "loss": 1.2784,
      "step": 1320
    },
    {
      "epoch": 4.783393501805055,
      "grad_norm": 1.6272616386413574,
      "learning_rate": 5.2166064981949456e-05,
      "loss": 1.3398,
      "step": 1325
    },
    {
      "epoch": 4.8014440433212995,
      "grad_norm": 1.783246636390686,
      "learning_rate": 5.1985559566787e-05,
      "loss": 1.2719,
      "step": 1330
    },
    {
      "epoch": 4.819494584837545,
      "grad_norm": 1.939771294593811,
      "learning_rate": 5.180505415162455e-05,
      "loss": 1.2914,
      "step": 1335
    },
    {
      "epoch": 4.837545126353791,
      "grad_norm": 1.8083915710449219,
      "learning_rate": 5.16245487364621e-05,
      "loss": 1.2982,
      "step": 1340
    },
    {
      "epoch": 4.855595667870036,
      "grad_norm": 1.8169958591461182,
      "learning_rate": 5.144404332129964e-05,
      "loss": 1.2733,
      "step": 1345
    },
    {
      "epoch": 4.873646209386282,
      "grad_norm": 1.862913966178894,
      "learning_rate": 5.126353790613718e-05,
      "loss": 1.3019,
      "step": 1350
    },
    {
      "epoch": 4.891696750902527,
      "grad_norm": 1.7534371614456177,
      "learning_rate": 5.108303249097473e-05,
      "loss": 1.3138,
      "step": 1355
    },
    {
      "epoch": 4.909747292418773,
      "grad_norm": 1.8779478073120117,
      "learning_rate": 5.090252707581228e-05,
      "loss": 1.3086,
      "step": 1360
    },
    {
      "epoch": 4.927797833935018,
      "grad_norm": 1.820001244544983,
      "learning_rate": 5.072202166064982e-05,
      "loss": 1.2986,
      "step": 1365
    },
    {
      "epoch": 4.945848375451264,
      "grad_norm": 1.8723207712173462,
      "learning_rate": 5.054151624548736e-05,
      "loss": 1.2499,
      "step": 1370
    },
    {
      "epoch": 4.963898916967509,
      "grad_norm": 1.8229717016220093,
      "learning_rate": 5.036101083032492e-05,
      "loss": 1.2726,
      "step": 1375
    },
    {
      "epoch": 4.981949458483754,
      "grad_norm": 1.9705311059951782,
      "learning_rate": 5.018050541516246e-05,
      "loss": 1.2705,
      "step": 1380
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.368675708770752,
      "learning_rate": 5e-05,
      "loss": 1.2599,
      "step": 1385
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.115155816078186,
      "eval_runtime": 109.1697,
      "eval_samples_per_second": 8.116,
      "eval_steps_per_second": 1.017,
      "step": 1385
    },
    {
      "epoch": 5.018050541516246,
      "grad_norm": 1.8215978145599365,
      "learning_rate": 4.981949458483755e-05,
      "loss": 1.2765,
      "step": 1390
    },
    {
      "epoch": 5.036101083032491,
      "grad_norm": 1.696581482887268,
      "learning_rate": 4.963898916967509e-05,
      "loss": 1.2558,
      "step": 1395
    },
    {
      "epoch": 5.054151624548736,
      "grad_norm": 1.842252492904663,
      "learning_rate": 4.945848375451264e-05,
      "loss": 1.2982,
      "step": 1400
    },
    {
      "epoch": 5.072202166064982,
      "grad_norm": 1.836277723312378,
      "learning_rate": 4.927797833935018e-05,
      "loss": 1.2771,
      "step": 1405
    },
    {
      "epoch": 5.090252707581228,
      "grad_norm": 1.866461157798767,
      "learning_rate": 4.909747292418773e-05,
      "loss": 1.3147,
      "step": 1410
    },
    {
      "epoch": 5.108303249097473,
      "grad_norm": 1.8844318389892578,
      "learning_rate": 4.891696750902527e-05,
      "loss": 1.2667,
      "step": 1415
    },
    {
      "epoch": 5.126353790613718,
      "grad_norm": 1.8582149744033813,
      "learning_rate": 4.873646209386282e-05,
      "loss": 1.2272,
      "step": 1420
    },
    {
      "epoch": 5.144404332129964,
      "grad_norm": 1.8796342611312866,
      "learning_rate": 4.855595667870036e-05,
      "loss": 1.3271,
      "step": 1425
    },
    {
      "epoch": 5.162454873646209,
      "grad_norm": 1.802476406097412,
      "learning_rate": 4.837545126353791e-05,
      "loss": 1.2765,
      "step": 1430
    },
    {
      "epoch": 5.180505415162455,
      "grad_norm": 2.0375571250915527,
      "learning_rate": 4.819494584837546e-05,
      "loss": 1.3059,
      "step": 1435
    },
    {
      "epoch": 5.1985559566787005,
      "grad_norm": 1.7779525518417358,
      "learning_rate": 4.8014440433213e-05,
      "loss": 1.2521,
      "step": 1440
    },
    {
      "epoch": 5.216606498194946,
      "grad_norm": 1.8456103801727295,
      "learning_rate": 4.783393501805055e-05,
      "loss": 1.3001,
      "step": 1445
    },
    {
      "epoch": 5.234657039711191,
      "grad_norm": 1.9354335069656372,
      "learning_rate": 4.765342960288809e-05,
      "loss": 1.2869,
      "step": 1450
    },
    {
      "epoch": 5.252707581227437,
      "grad_norm": 1.7938833236694336,
      "learning_rate": 4.747292418772563e-05,
      "loss": 1.2569,
      "step": 1455
    },
    {
      "epoch": 5.270758122743683,
      "grad_norm": 1.8270529508590698,
      "learning_rate": 4.7292418772563177e-05,
      "loss": 1.2799,
      "step": 1460
    },
    {
      "epoch": 5.2888086642599275,
      "grad_norm": 1.7483738660812378,
      "learning_rate": 4.711191335740072e-05,
      "loss": 1.29,
      "step": 1465
    },
    {
      "epoch": 5.306859205776173,
      "grad_norm": 1.8748575448989868,
      "learning_rate": 4.693140794223827e-05,
      "loss": 1.3279,
      "step": 1470
    },
    {
      "epoch": 5.324909747292419,
      "grad_norm": 1.7418277263641357,
      "learning_rate": 4.675090252707581e-05,
      "loss": 1.2689,
      "step": 1475
    },
    {
      "epoch": 5.342960288808664,
      "grad_norm": 1.7748775482177734,
      "learning_rate": 4.657039711191336e-05,
      "loss": 1.2628,
      "step": 1480
    },
    {
      "epoch": 5.3610108303249095,
      "grad_norm": 1.9051263332366943,
      "learning_rate": 4.63898916967509e-05,
      "loss": 1.2652,
      "step": 1485
    },
    {
      "epoch": 5.379061371841155,
      "grad_norm": 1.977478265762329,
      "learning_rate": 4.620938628158845e-05,
      "loss": 1.233,
      "step": 1490
    },
    {
      "epoch": 5.397111913357401,
      "grad_norm": 1.855549931526184,
      "learning_rate": 4.602888086642599e-05,
      "loss": 1.277,
      "step": 1495
    },
    {
      "epoch": 5.415162454873646,
      "grad_norm": 1.773913025856018,
      "learning_rate": 4.584837545126354e-05,
      "loss": 1.2959,
      "step": 1500
    },
    {
      "epoch": 5.433212996389892,
      "grad_norm": 1.9239327907562256,
      "learning_rate": 4.566787003610109e-05,
      "loss": 1.292,
      "step": 1505
    },
    {
      "epoch": 5.451263537906137,
      "grad_norm": 1.8644661903381348,
      "learning_rate": 4.548736462093863e-05,
      "loss": 1.2715,
      "step": 1510
    },
    {
      "epoch": 5.469314079422382,
      "grad_norm": 1.9324641227722168,
      "learning_rate": 4.530685920577618e-05,
      "loss": 1.2462,
      "step": 1515
    },
    {
      "epoch": 5.487364620938628,
      "grad_norm": 1.7653629779815674,
      "learning_rate": 4.5126353790613716e-05,
      "loss": 1.2933,
      "step": 1520
    },
    {
      "epoch": 5.505415162454874,
      "grad_norm": 1.7958225011825562,
      "learning_rate": 4.494584837545127e-05,
      "loss": 1.2518,
      "step": 1525
    },
    {
      "epoch": 5.5234657039711195,
      "grad_norm": 1.8248600959777832,
      "learning_rate": 4.4765342960288806e-05,
      "loss": 1.2432,
      "step": 1530
    },
    {
      "epoch": 5.541516245487364,
      "grad_norm": 1.7991950511932373,
      "learning_rate": 4.458483754512636e-05,
      "loss": 1.286,
      "step": 1535
    },
    {
      "epoch": 5.55956678700361,
      "grad_norm": 1.8712502717971802,
      "learning_rate": 4.44043321299639e-05,
      "loss": 1.26,
      "step": 1540
    },
    {
      "epoch": 5.577617328519856,
      "grad_norm": 1.8301557302474976,
      "learning_rate": 4.422382671480145e-05,
      "loss": 1.3062,
      "step": 1545
    },
    {
      "epoch": 5.595667870036101,
      "grad_norm": 1.822576880455017,
      "learning_rate": 4.404332129963899e-05,
      "loss": 1.2634,
      "step": 1550
    },
    {
      "epoch": 5.613718411552346,
      "grad_norm": 1.8894622325897217,
      "learning_rate": 4.386281588447654e-05,
      "loss": 1.2589,
      "step": 1555
    },
    {
      "epoch": 5.631768953068592,
      "grad_norm": 1.8156834840774536,
      "learning_rate": 4.368231046931408e-05,
      "loss": 1.3064,
      "step": 1560
    },
    {
      "epoch": 5.649819494584838,
      "grad_norm": 1.8180533647537231,
      "learning_rate": 4.350180505415163e-05,
      "loss": 1.2214,
      "step": 1565
    },
    {
      "epoch": 5.667870036101083,
      "grad_norm": 1.800431728363037,
      "learning_rate": 4.332129963898917e-05,
      "loss": 1.29,
      "step": 1570
    },
    {
      "epoch": 5.6859205776173285,
      "grad_norm": 1.9362859725952148,
      "learning_rate": 4.314079422382672e-05,
      "loss": 1.2831,
      "step": 1575
    },
    {
      "epoch": 5.703971119133574,
      "grad_norm": 2.0691516399383545,
      "learning_rate": 4.296028880866426e-05,
      "loss": 1.241,
      "step": 1580
    },
    {
      "epoch": 5.722021660649819,
      "grad_norm": 1.8698318004608154,
      "learning_rate": 4.277978339350181e-05,
      "loss": 1.2929,
      "step": 1585
    },
    {
      "epoch": 5.740072202166065,
      "grad_norm": 1.7521082162857056,
      "learning_rate": 4.259927797833935e-05,
      "loss": 1.3144,
      "step": 1590
    },
    {
      "epoch": 5.758122743682311,
      "grad_norm": 1.8873845338821411,
      "learning_rate": 4.24187725631769e-05,
      "loss": 1.2785,
      "step": 1595
    },
    {
      "epoch": 5.776173285198556,
      "grad_norm": 1.7942394018173218,
      "learning_rate": 4.223826714801444e-05,
      "loss": 1.2475,
      "step": 1600
    },
    {
      "epoch": 5.794223826714801,
      "grad_norm": 1.8146040439605713,
      "learning_rate": 4.205776173285199e-05,
      "loss": 1.2452,
      "step": 1605
    },
    {
      "epoch": 5.812274368231047,
      "grad_norm": 1.8747730255126953,
      "learning_rate": 4.187725631768953e-05,
      "loss": 1.2721,
      "step": 1610
    },
    {
      "epoch": 5.830324909747293,
      "grad_norm": 1.7559877634048462,
      "learning_rate": 4.169675090252708e-05,
      "loss": 1.3464,
      "step": 1615
    },
    {
      "epoch": 5.8483754512635375,
      "grad_norm": 1.7824268341064453,
      "learning_rate": 4.151624548736462e-05,
      "loss": 1.2496,
      "step": 1620
    },
    {
      "epoch": 5.866425992779783,
      "grad_norm": 1.8544588088989258,
      "learning_rate": 4.1335740072202167e-05,
      "loss": 1.2819,
      "step": 1625
    },
    {
      "epoch": 5.884476534296029,
      "grad_norm": 1.9978258609771729,
      "learning_rate": 4.115523465703972e-05,
      "loss": 1.2324,
      "step": 1630
    },
    {
      "epoch": 5.902527075812275,
      "grad_norm": 1.787343978881836,
      "learning_rate": 4.0974729241877256e-05,
      "loss": 1.2594,
      "step": 1635
    },
    {
      "epoch": 5.92057761732852,
      "grad_norm": 1.956587791442871,
      "learning_rate": 4.079422382671481e-05,
      "loss": 1.2383,
      "step": 1640
    },
    {
      "epoch": 5.938628158844765,
      "grad_norm": 1.953301191329956,
      "learning_rate": 4.0613718411552346e-05,
      "loss": 1.2507,
      "step": 1645
    },
    {
      "epoch": 5.956678700361011,
      "grad_norm": 1.8236902952194214,
      "learning_rate": 4.043321299638989e-05,
      "loss": 1.2508,
      "step": 1650
    },
    {
      "epoch": 5.974729241877256,
      "grad_norm": 1.8802400827407837,
      "learning_rate": 4.0252707581227436e-05,
      "loss": 1.265,
      "step": 1655
    },
    {
      "epoch": 5.992779783393502,
      "grad_norm": 1.7802693843841553,
      "learning_rate": 4.007220216606498e-05,
      "loss": 1.2979,
      "step": 1660
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.0998107194900513,
      "eval_runtime": 109.3248,
      "eval_samples_per_second": 8.104,
      "eval_steps_per_second": 1.015,
      "step": 1662
    },
    {
      "epoch": 6.0108303249097474,
      "grad_norm": 1.9371851682662964,
      "learning_rate": 3.989169675090253e-05,
      "loss": 1.2561,
      "step": 1665
    },
    {
      "epoch": 6.028880866425993,
      "grad_norm": 1.7296359539031982,
      "learning_rate": 3.971119133574007e-05,
      "loss": 1.2275,
      "step": 1670
    },
    {
      "epoch": 6.046931407942238,
      "grad_norm": 1.8543429374694824,
      "learning_rate": 3.953068592057762e-05,
      "loss": 1.3342,
      "step": 1675
    },
    {
      "epoch": 6.064981949458484,
      "grad_norm": 1.859803318977356,
      "learning_rate": 3.935018050541516e-05,
      "loss": 1.2262,
      "step": 1680
    },
    {
      "epoch": 6.0830324909747295,
      "grad_norm": 1.7468430995941162,
      "learning_rate": 3.916967509025271e-05,
      "loss": 1.2454,
      "step": 1685
    },
    {
      "epoch": 6.101083032490974,
      "grad_norm": 1.8010965585708618,
      "learning_rate": 3.898916967509025e-05,
      "loss": 1.2369,
      "step": 1690
    },
    {
      "epoch": 6.11913357400722,
      "grad_norm": 1.8860886096954346,
      "learning_rate": 3.88086642599278e-05,
      "loss": 1.2808,
      "step": 1695
    },
    {
      "epoch": 6.137184115523466,
      "grad_norm": 1.8643522262573242,
      "learning_rate": 3.862815884476535e-05,
      "loss": 1.281,
      "step": 1700
    },
    {
      "epoch": 6.155234657039712,
      "grad_norm": 2.011537551879883,
      "learning_rate": 3.844765342960289e-05,
      "loss": 1.263,
      "step": 1705
    },
    {
      "epoch": 6.1732851985559565,
      "grad_norm": 1.7031854391098022,
      "learning_rate": 3.826714801444044e-05,
      "loss": 1.3064,
      "step": 1710
    },
    {
      "epoch": 6.191335740072202,
      "grad_norm": 1.808018684387207,
      "learning_rate": 3.8086642599277976e-05,
      "loss": 1.2319,
      "step": 1715
    },
    {
      "epoch": 6.209386281588448,
      "grad_norm": 2.183945417404175,
      "learning_rate": 3.790613718411553e-05,
      "loss": 1.2672,
      "step": 1720
    },
    {
      "epoch": 6.227436823104693,
      "grad_norm": 1.8874000310897827,
      "learning_rate": 3.7725631768953066e-05,
      "loss": 1.2476,
      "step": 1725
    },
    {
      "epoch": 6.245487364620939,
      "grad_norm": 1.8552137613296509,
      "learning_rate": 3.754512635379062e-05,
      "loss": 1.2596,
      "step": 1730
    },
    {
      "epoch": 6.263537906137184,
      "grad_norm": 1.9315087795257568,
      "learning_rate": 3.7364620938628155e-05,
      "loss": 1.2408,
      "step": 1735
    },
    {
      "epoch": 6.28158844765343,
      "grad_norm": 1.8983904123306274,
      "learning_rate": 3.718411552346571e-05,
      "loss": 1.2797,
      "step": 1740
    },
    {
      "epoch": 6.299638989169675,
      "grad_norm": 2.0369210243225098,
      "learning_rate": 3.700361010830325e-05,
      "loss": 1.2418,
      "step": 1745
    },
    {
      "epoch": 6.317689530685921,
      "grad_norm": 2.0033771991729736,
      "learning_rate": 3.68231046931408e-05,
      "loss": 1.3059,
      "step": 1750
    },
    {
      "epoch": 6.335740072202166,
      "grad_norm": 1.7680789232254028,
      "learning_rate": 3.664259927797834e-05,
      "loss": 1.2037,
      "step": 1755
    },
    {
      "epoch": 6.353790613718411,
      "grad_norm": 2.010519504547119,
      "learning_rate": 3.646209386281589e-05,
      "loss": 1.2513,
      "step": 1760
    },
    {
      "epoch": 6.371841155234657,
      "grad_norm": 1.8029917478561401,
      "learning_rate": 3.628158844765343e-05,
      "loss": 1.2997,
      "step": 1765
    },
    {
      "epoch": 6.389891696750903,
      "grad_norm": 1.9991974830627441,
      "learning_rate": 3.610108303249098e-05,
      "loss": 1.2084,
      "step": 1770
    },
    {
      "epoch": 6.4079422382671485,
      "grad_norm": 1.827112078666687,
      "learning_rate": 3.592057761732852e-05,
      "loss": 1.2403,
      "step": 1775
    },
    {
      "epoch": 6.425992779783393,
      "grad_norm": 1.8415381908416748,
      "learning_rate": 3.574007220216607e-05,
      "loss": 1.2674,
      "step": 1780
    },
    {
      "epoch": 6.444043321299639,
      "grad_norm": 1.8755244016647339,
      "learning_rate": 3.555956678700361e-05,
      "loss": 1.2641,
      "step": 1785
    },
    {
      "epoch": 6.462093862815885,
      "grad_norm": 1.89704430103302,
      "learning_rate": 3.537906137184116e-05,
      "loss": 1.2835,
      "step": 1790
    },
    {
      "epoch": 6.48014440433213,
      "grad_norm": 2.0007216930389404,
      "learning_rate": 3.51985559566787e-05,
      "loss": 1.3001,
      "step": 1795
    },
    {
      "epoch": 6.498194945848375,
      "grad_norm": 2.044006824493408,
      "learning_rate": 3.5018050541516247e-05,
      "loss": 1.2512,
      "step": 1800
    },
    {
      "epoch": 6.516245487364621,
      "grad_norm": 2.0268032550811768,
      "learning_rate": 3.483754512635379e-05,
      "loss": 1.2484,
      "step": 1805
    },
    {
      "epoch": 6.534296028880867,
      "grad_norm": 1.8055115938186646,
      "learning_rate": 3.4657039711191336e-05,
      "loss": 1.2795,
      "step": 1810
    },
    {
      "epoch": 6.552346570397112,
      "grad_norm": 1.9044921398162842,
      "learning_rate": 3.447653429602888e-05,
      "loss": 1.2352,
      "step": 1815
    },
    {
      "epoch": 6.5703971119133575,
      "grad_norm": 1.8867460489273071,
      "learning_rate": 3.4296028880866426e-05,
      "loss": 1.2713,
      "step": 1820
    },
    {
      "epoch": 6.588447653429603,
      "grad_norm": 1.7427246570587158,
      "learning_rate": 3.411552346570397e-05,
      "loss": 1.2643,
      "step": 1825
    },
    {
      "epoch": 6.606498194945848,
      "grad_norm": 1.7624911069869995,
      "learning_rate": 3.3935018050541516e-05,
      "loss": 1.2479,
      "step": 1830
    },
    {
      "epoch": 6.624548736462094,
      "grad_norm": 1.864248514175415,
      "learning_rate": 3.375451263537907e-05,
      "loss": 1.2573,
      "step": 1835
    },
    {
      "epoch": 6.64259927797834,
      "grad_norm": 2.075676918029785,
      "learning_rate": 3.3574007220216606e-05,
      "loss": 1.2854,
      "step": 1840
    },
    {
      "epoch": 6.6606498194945845,
      "grad_norm": 1.9594027996063232,
      "learning_rate": 3.339350180505416e-05,
      "loss": 1.2679,
      "step": 1845
    },
    {
      "epoch": 6.67870036101083,
      "grad_norm": 1.9112980365753174,
      "learning_rate": 3.3212996389891696e-05,
      "loss": 1.2341,
      "step": 1850
    },
    {
      "epoch": 6.696750902527076,
      "grad_norm": 1.9745075702667236,
      "learning_rate": 3.303249097472924e-05,
      "loss": 1.2656,
      "step": 1855
    },
    {
      "epoch": 6.714801444043322,
      "grad_norm": 1.9387378692626953,
      "learning_rate": 3.2851985559566786e-05,
      "loss": 1.255,
      "step": 1860
    },
    {
      "epoch": 6.7328519855595665,
      "grad_norm": 2.070927143096924,
      "learning_rate": 3.267148014440433e-05,
      "loss": 1.2239,
      "step": 1865
    },
    {
      "epoch": 6.750902527075812,
      "grad_norm": 1.9613018035888672,
      "learning_rate": 3.249097472924188e-05,
      "loss": 1.235,
      "step": 1870
    },
    {
      "epoch": 6.768953068592058,
      "grad_norm": 1.9112977981567383,
      "learning_rate": 3.231046931407942e-05,
      "loss": 1.2762,
      "step": 1875
    },
    {
      "epoch": 6.787003610108303,
      "grad_norm": 1.89004647731781,
      "learning_rate": 3.212996389891697e-05,
      "loss": 1.2458,
      "step": 1880
    },
    {
      "epoch": 6.805054151624549,
      "grad_norm": 1.8168998956680298,
      "learning_rate": 3.194945848375451e-05,
      "loss": 1.2552,
      "step": 1885
    },
    {
      "epoch": 6.823104693140794,
      "grad_norm": 1.8816477060317993,
      "learning_rate": 3.176895306859206e-05,
      "loss": 1.2557,
      "step": 1890
    },
    {
      "epoch": 6.841155234657039,
      "grad_norm": 2.037916898727417,
      "learning_rate": 3.15884476534296e-05,
      "loss": 1.2727,
      "step": 1895
    },
    {
      "epoch": 6.859205776173285,
      "grad_norm": 2.0088586807250977,
      "learning_rate": 3.140794223826715e-05,
      "loss": 1.2575,
      "step": 1900
    },
    {
      "epoch": 6.877256317689531,
      "grad_norm": 1.8681118488311768,
      "learning_rate": 3.12274368231047e-05,
      "loss": 1.2379,
      "step": 1905
    },
    {
      "epoch": 6.8953068592057765,
      "grad_norm": 1.7749403715133667,
      "learning_rate": 3.104693140794224e-05,
      "loss": 1.2929,
      "step": 1910
    },
    {
      "epoch": 6.913357400722021,
      "grad_norm": 1.9863766431808472,
      "learning_rate": 3.086642599277979e-05,
      "loss": 1.2135,
      "step": 1915
    },
    {
      "epoch": 6.931407942238267,
      "grad_norm": 1.871976375579834,
      "learning_rate": 3.0685920577617325e-05,
      "loss": 1.241,
      "step": 1920
    },
    {
      "epoch": 6.949458483754513,
      "grad_norm": 2.0070712566375732,
      "learning_rate": 3.0505415162454877e-05,
      "loss": 1.253,
      "step": 1925
    },
    {
      "epoch": 6.967509025270758,
      "grad_norm": 2.051499128341675,
      "learning_rate": 3.032490974729242e-05,
      "loss": 1.2285,
      "step": 1930
    },
    {
      "epoch": 6.985559566787003,
      "grad_norm": 2.0395894050598145,
      "learning_rate": 3.0144404332129967e-05,
      "loss": 1.2426,
      "step": 1935
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.0888378620147705,
      "eval_runtime": 109.3949,
      "eval_samples_per_second": 8.099,
      "eval_steps_per_second": 1.015,
      "step": 1939
    },
    {
      "epoch": 7.003610108303249,
      "grad_norm": 2.0813121795654297,
      "learning_rate": 2.996389891696751e-05,
      "loss": 1.2572,
      "step": 1940
    },
    {
      "epoch": 7.021660649819495,
      "grad_norm": 2.106574773788452,
      "learning_rate": 2.9783393501805057e-05,
      "loss": 1.2354,
      "step": 1945
    },
    {
      "epoch": 7.03971119133574,
      "grad_norm": 1.8254168033599854,
      "learning_rate": 2.9602888086642598e-05,
      "loss": 1.2395,
      "step": 1950
    },
    {
      "epoch": 7.0577617328519855,
      "grad_norm": 2.011394739151001,
      "learning_rate": 2.9422382671480147e-05,
      "loss": 1.2711,
      "step": 1955
    },
    {
      "epoch": 7.075812274368231,
      "grad_norm": 1.9731706380844116,
      "learning_rate": 2.924187725631769e-05,
      "loss": 1.2609,
      "step": 1960
    },
    {
      "epoch": 7.093862815884476,
      "grad_norm": 1.810958743095398,
      "learning_rate": 2.906137184115524e-05,
      "loss": 1.2401,
      "step": 1965
    },
    {
      "epoch": 7.111913357400722,
      "grad_norm": 1.8422091007232666,
      "learning_rate": 2.888086642599278e-05,
      "loss": 1.2483,
      "step": 1970
    },
    {
      "epoch": 7.129963898916968,
      "grad_norm": 1.968073844909668,
      "learning_rate": 2.870036101083033e-05,
      "loss": 1.2439,
      "step": 1975
    },
    {
      "epoch": 7.148014440433213,
      "grad_norm": 1.8425406217575073,
      "learning_rate": 2.851985559566787e-05,
      "loss": 1.1963,
      "step": 1980
    },
    {
      "epoch": 7.166064981949458,
      "grad_norm": 1.882514476776123,
      "learning_rate": 2.8339350180505413e-05,
      "loss": 1.2687,
      "step": 1985
    },
    {
      "epoch": 7.184115523465704,
      "grad_norm": 1.7833706140518188,
      "learning_rate": 2.815884476534296e-05,
      "loss": 1.2373,
      "step": 1990
    },
    {
      "epoch": 7.20216606498195,
      "grad_norm": 1.9092180728912354,
      "learning_rate": 2.7978339350180506e-05,
      "loss": 1.2668,
      "step": 1995
    },
    {
      "epoch": 7.2202166064981945,
      "grad_norm": 1.885539174079895,
      "learning_rate": 2.779783393501805e-05,
      "loss": 1.1976,
      "step": 2000
    },
    {
      "epoch": 7.23826714801444,
      "grad_norm": 2.150563955307007,
      "learning_rate": 2.7617328519855596e-05,
      "loss": 1.2498,
      "step": 2005
    },
    {
      "epoch": 7.256317689530686,
      "grad_norm": 2.0806806087493896,
      "learning_rate": 2.7436823104693144e-05,
      "loss": 1.2441,
      "step": 2010
    },
    {
      "epoch": 7.274368231046932,
      "grad_norm": 2.12005615234375,
      "learning_rate": 2.7256317689530686e-05,
      "loss": 1.235,
      "step": 2015
    },
    {
      "epoch": 7.292418772563177,
      "grad_norm": 1.9456616640090942,
      "learning_rate": 2.7075812274368234e-05,
      "loss": 1.235,
      "step": 2020
    },
    {
      "epoch": 7.310469314079422,
      "grad_norm": 1.8829705715179443,
      "learning_rate": 2.6895306859205776e-05,
      "loss": 1.2691,
      "step": 2025
    },
    {
      "epoch": 7.328519855595668,
      "grad_norm": 1.8921070098876953,
      "learning_rate": 2.6714801444043324e-05,
      "loss": 1.2223,
      "step": 2030
    },
    {
      "epoch": 7.346570397111913,
      "grad_norm": 1.838112235069275,
      "learning_rate": 2.6534296028880866e-05,
      "loss": 1.2702,
      "step": 2035
    },
    {
      "epoch": 7.364620938628159,
      "grad_norm": 1.886906385421753,
      "learning_rate": 2.6353790613718414e-05,
      "loss": 1.2348,
      "step": 2040
    },
    {
      "epoch": 7.382671480144404,
      "grad_norm": 2.0309667587280273,
      "learning_rate": 2.617328519855596e-05,
      "loss": 1.2379,
      "step": 2045
    },
    {
      "epoch": 7.40072202166065,
      "grad_norm": 1.9977353811264038,
      "learning_rate": 2.59927797833935e-05,
      "loss": 1.2108,
      "step": 2050
    },
    {
      "epoch": 7.418772563176895,
      "grad_norm": 1.957729458808899,
      "learning_rate": 2.581227436823105e-05,
      "loss": 1.2757,
      "step": 2055
    },
    {
      "epoch": 7.436823104693141,
      "grad_norm": 2.1365139484405518,
      "learning_rate": 2.563176895306859e-05,
      "loss": 1.284,
      "step": 2060
    },
    {
      "epoch": 7.4548736462093865,
      "grad_norm": 1.9307901859283447,
      "learning_rate": 2.545126353790614e-05,
      "loss": 1.2625,
      "step": 2065
    },
    {
      "epoch": 7.472924187725631,
      "grad_norm": 1.8583658933639526,
      "learning_rate": 2.527075812274368e-05,
      "loss": 1.3062,
      "step": 2070
    },
    {
      "epoch": 7.490974729241877,
      "grad_norm": 1.9620578289031982,
      "learning_rate": 2.509025270758123e-05,
      "loss": 1.2204,
      "step": 2075
    },
    {
      "epoch": 7.509025270758123,
      "grad_norm": 1.9070758819580078,
      "learning_rate": 2.4909747292418774e-05,
      "loss": 1.2587,
      "step": 2080
    },
    {
      "epoch": 7.527075812274369,
      "grad_norm": 2.043083429336548,
      "learning_rate": 2.472924187725632e-05,
      "loss": 1.2395,
      "step": 2085
    },
    {
      "epoch": 7.5451263537906135,
      "grad_norm": 1.9275797605514526,
      "learning_rate": 2.4548736462093864e-05,
      "loss": 1.266,
      "step": 2090
    },
    {
      "epoch": 7.563176895306859,
      "grad_norm": 1.9088318347930908,
      "learning_rate": 2.436823104693141e-05,
      "loss": 1.2714,
      "step": 2095
    },
    {
      "epoch": 7.581227436823105,
      "grad_norm": 2.0033814907073975,
      "learning_rate": 2.4187725631768953e-05,
      "loss": 1.2423,
      "step": 2100
    },
    {
      "epoch": 7.59927797833935,
      "grad_norm": 1.9146968126296997,
      "learning_rate": 2.40072202166065e-05,
      "loss": 1.2484,
      "step": 2105
    },
    {
      "epoch": 7.617328519855596,
      "grad_norm": 2.0004217624664307,
      "learning_rate": 2.3826714801444043e-05,
      "loss": 1.2505,
      "step": 2110
    },
    {
      "epoch": 7.635379061371841,
      "grad_norm": 2.001182794570923,
      "learning_rate": 2.3646209386281588e-05,
      "loss": 1.3213,
      "step": 2115
    },
    {
      "epoch": 7.653429602888087,
      "grad_norm": 1.8628554344177246,
      "learning_rate": 2.3465703971119137e-05,
      "loss": 1.2579,
      "step": 2120
    },
    {
      "epoch": 7.671480144404332,
      "grad_norm": 2.0298540592193604,
      "learning_rate": 2.328519855595668e-05,
      "loss": 1.201,
      "step": 2125
    },
    {
      "epoch": 7.689530685920578,
      "grad_norm": 2.044501781463623,
      "learning_rate": 2.3104693140794227e-05,
      "loss": 1.2055,
      "step": 2130
    },
    {
      "epoch": 7.707581227436823,
      "grad_norm": 2.0123298168182373,
      "learning_rate": 2.292418772563177e-05,
      "loss": 1.2538,
      "step": 2135
    },
    {
      "epoch": 7.725631768953068,
      "grad_norm": 1.9416521787643433,
      "learning_rate": 2.2743682310469316e-05,
      "loss": 1.2757,
      "step": 2140
    },
    {
      "epoch": 7.743682310469314,
      "grad_norm": 1.8941373825073242,
      "learning_rate": 2.2563176895306858e-05,
      "loss": 1.2369,
      "step": 2145
    },
    {
      "epoch": 7.76173285198556,
      "grad_norm": 1.913818597793579,
      "learning_rate": 2.2382671480144403e-05,
      "loss": 1.2309,
      "step": 2150
    },
    {
      "epoch": 7.7797833935018055,
      "grad_norm": 1.9639027118682861,
      "learning_rate": 2.220216606498195e-05,
      "loss": 1.2659,
      "step": 2155
    },
    {
      "epoch": 7.79783393501805,
      "grad_norm": 1.818530559539795,
      "learning_rate": 2.2021660649819496e-05,
      "loss": 1.2624,
      "step": 2160
    },
    {
      "epoch": 7.815884476534296,
      "grad_norm": 1.9210014343261719,
      "learning_rate": 2.184115523465704e-05,
      "loss": 1.1972,
      "step": 2165
    },
    {
      "epoch": 7.833935018050542,
      "grad_norm": 1.862196683883667,
      "learning_rate": 2.1660649819494586e-05,
      "loss": 1.2304,
      "step": 2170
    },
    {
      "epoch": 7.851985559566787,
      "grad_norm": 1.965844750404358,
      "learning_rate": 2.148014440433213e-05,
      "loss": 1.2779,
      "step": 2175
    },
    {
      "epoch": 7.870036101083032,
      "grad_norm": 2.0133559703826904,
      "learning_rate": 2.1299638989169676e-05,
      "loss": 1.2598,
      "step": 2180
    },
    {
      "epoch": 7.888086642599278,
      "grad_norm": 1.960439682006836,
      "learning_rate": 2.111913357400722e-05,
      "loss": 1.2201,
      "step": 2185
    },
    {
      "epoch": 7.906137184115524,
      "grad_norm": 1.809314250946045,
      "learning_rate": 2.0938628158844766e-05,
      "loss": 1.2787,
      "step": 2190
    },
    {
      "epoch": 7.924187725631769,
      "grad_norm": 2.0357236862182617,
      "learning_rate": 2.075812274368231e-05,
      "loss": 1.2586,
      "step": 2195
    },
    {
      "epoch": 7.9422382671480145,
      "grad_norm": 2.0252814292907715,
      "learning_rate": 2.057761732851986e-05,
      "loss": 1.2508,
      "step": 2200
    },
    {
      "epoch": 7.96028880866426,
      "grad_norm": 1.9856696128845215,
      "learning_rate": 2.0397111913357404e-05,
      "loss": 1.2423,
      "step": 2205
    },
    {
      "epoch": 7.978339350180505,
      "grad_norm": 2.149388313293457,
      "learning_rate": 2.0216606498194946e-05,
      "loss": 1.2358,
      "step": 2210
    },
    {
      "epoch": 7.996389891696751,
      "grad_norm": 1.9730745553970337,
      "learning_rate": 2.003610108303249e-05,
      "loss": 1.2346,
      "step": 2215
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.0825275182724,
      "eval_runtime": 109.3784,
      "eval_samples_per_second": 8.1,
      "eval_steps_per_second": 1.015,
      "step": 2216
    },
    {
      "epoch": 8.014440433212997,
      "grad_norm": 1.9929851293563843,
      "learning_rate": 1.9855595667870036e-05,
      "loss": 1.2261,
      "step": 2220
    },
    {
      "epoch": 8.032490974729242,
      "grad_norm": 1.8699705600738525,
      "learning_rate": 1.967509025270758e-05,
      "loss": 1.2353,
      "step": 2225
    },
    {
      "epoch": 8.050541516245488,
      "grad_norm": 2.1220595836639404,
      "learning_rate": 1.9494584837545125e-05,
      "loss": 1.2304,
      "step": 2230
    },
    {
      "epoch": 8.068592057761732,
      "grad_norm": 1.7737913131713867,
      "learning_rate": 1.9314079422382674e-05,
      "loss": 1.2455,
      "step": 2235
    },
    {
      "epoch": 8.086642599277978,
      "grad_norm": 2.0221662521362305,
      "learning_rate": 1.913357400722022e-05,
      "loss": 1.2624,
      "step": 2240
    },
    {
      "epoch": 8.104693140794224,
      "grad_norm": 1.7866262197494507,
      "learning_rate": 1.8953068592057764e-05,
      "loss": 1.2372,
      "step": 2245
    },
    {
      "epoch": 8.12274368231047,
      "grad_norm": 1.8085641860961914,
      "learning_rate": 1.877256317689531e-05,
      "loss": 1.2448,
      "step": 2250
    },
    {
      "epoch": 8.140794223826715,
      "grad_norm": 1.9795687198638916,
      "learning_rate": 1.8592057761732854e-05,
      "loss": 1.2615,
      "step": 2255
    },
    {
      "epoch": 8.15884476534296,
      "grad_norm": 1.9477777481079102,
      "learning_rate": 1.84115523465704e-05,
      "loss": 1.2396,
      "step": 2260
    },
    {
      "epoch": 8.176895306859207,
      "grad_norm": 1.9787323474884033,
      "learning_rate": 1.8231046931407943e-05,
      "loss": 1.2355,
      "step": 2265
    },
    {
      "epoch": 8.19494584837545,
      "grad_norm": 1.9953161478042603,
      "learning_rate": 1.805054151624549e-05,
      "loss": 1.2273,
      "step": 2270
    },
    {
      "epoch": 8.212996389891696,
      "grad_norm": 2.066664457321167,
      "learning_rate": 1.7870036101083033e-05,
      "loss": 1.2445,
      "step": 2275
    },
    {
      "epoch": 8.231046931407942,
      "grad_norm": 1.8790698051452637,
      "learning_rate": 1.768953068592058e-05,
      "loss": 1.2106,
      "step": 2280
    },
    {
      "epoch": 8.249097472924188,
      "grad_norm": 1.9133052825927734,
      "learning_rate": 1.7509025270758123e-05,
      "loss": 1.2376,
      "step": 2285
    },
    {
      "epoch": 8.267148014440433,
      "grad_norm": 2.247183084487915,
      "learning_rate": 1.7328519855595668e-05,
      "loss": 1.2534,
      "step": 2290
    },
    {
      "epoch": 8.28519855595668,
      "grad_norm": 1.9414348602294922,
      "learning_rate": 1.7148014440433213e-05,
      "loss": 1.2341,
      "step": 2295
    },
    {
      "epoch": 8.303249097472925,
      "grad_norm": 1.9407583475112915,
      "learning_rate": 1.6967509025270758e-05,
      "loss": 1.2097,
      "step": 2300
    },
    {
      "epoch": 8.321299638989169,
      "grad_norm": 1.8687806129455566,
      "learning_rate": 1.6787003610108303e-05,
      "loss": 1.2546,
      "step": 2305
    },
    {
      "epoch": 8.339350180505415,
      "grad_norm": 1.88034188747406,
      "learning_rate": 1.6606498194945848e-05,
      "loss": 1.193,
      "step": 2310
    },
    {
      "epoch": 8.35740072202166,
      "grad_norm": 2.005556344985962,
      "learning_rate": 1.6425992779783393e-05,
      "loss": 1.2292,
      "step": 2315
    },
    {
      "epoch": 8.375451263537906,
      "grad_norm": 1.962579369544983,
      "learning_rate": 1.624548736462094e-05,
      "loss": 1.2331,
      "step": 2320
    },
    {
      "epoch": 8.393501805054152,
      "grad_norm": 1.8952430486679077,
      "learning_rate": 1.6064981949458486e-05,
      "loss": 1.2209,
      "step": 2325
    },
    {
      "epoch": 8.411552346570398,
      "grad_norm": 2.2548882961273193,
      "learning_rate": 1.588447653429603e-05,
      "loss": 1.2457,
      "step": 2330
    },
    {
      "epoch": 8.429602888086643,
      "grad_norm": 2.0167617797851562,
      "learning_rate": 1.5703971119133576e-05,
      "loss": 1.188,
      "step": 2335
    },
    {
      "epoch": 8.447653429602887,
      "grad_norm": 2.052611827850342,
      "learning_rate": 1.552346570397112e-05,
      "loss": 1.2457,
      "step": 2340
    },
    {
      "epoch": 8.465703971119133,
      "grad_norm": 1.95607328414917,
      "learning_rate": 1.5342960288808663e-05,
      "loss": 1.2513,
      "step": 2345
    },
    {
      "epoch": 8.483754512635379,
      "grad_norm": 1.9369815587997437,
      "learning_rate": 1.516245487364621e-05,
      "loss": 1.2474,
      "step": 2350
    },
    {
      "epoch": 8.501805054151625,
      "grad_norm": 1.9470064640045166,
      "learning_rate": 1.4981949458483754e-05,
      "loss": 1.2613,
      "step": 2355
    },
    {
      "epoch": 8.51985559566787,
      "grad_norm": 1.8332690000534058,
      "learning_rate": 1.4801444043321299e-05,
      "loss": 1.2664,
      "step": 2360
    },
    {
      "epoch": 8.537906137184116,
      "grad_norm": 2.031536340713501,
      "learning_rate": 1.4620938628158846e-05,
      "loss": 1.2865,
      "step": 2365
    },
    {
      "epoch": 8.555956678700362,
      "grad_norm": 2.122921943664551,
      "learning_rate": 1.444043321299639e-05,
      "loss": 1.2032,
      "step": 2370
    },
    {
      "epoch": 8.574007220216606,
      "grad_norm": 1.8201994895935059,
      "learning_rate": 1.4259927797833936e-05,
      "loss": 1.251,
      "step": 2375
    },
    {
      "epoch": 8.592057761732852,
      "grad_norm": 2.194579601287842,
      "learning_rate": 1.407942238267148e-05,
      "loss": 1.2463,
      "step": 2380
    },
    {
      "epoch": 8.610108303249097,
      "grad_norm": 2.0981264114379883,
      "learning_rate": 1.3898916967509026e-05,
      "loss": 1.2175,
      "step": 2385
    },
    {
      "epoch": 8.628158844765343,
      "grad_norm": 2.0179333686828613,
      "learning_rate": 1.3718411552346572e-05,
      "loss": 1.2818,
      "step": 2390
    },
    {
      "epoch": 8.646209386281589,
      "grad_norm": 2.140320062637329,
      "learning_rate": 1.3537906137184117e-05,
      "loss": 1.2357,
      "step": 2395
    },
    {
      "epoch": 8.664259927797834,
      "grad_norm": 1.954353928565979,
      "learning_rate": 1.3357400722021662e-05,
      "loss": 1.2299,
      "step": 2400
    },
    {
      "epoch": 8.68231046931408,
      "grad_norm": 2.0632407665252686,
      "learning_rate": 1.3176895306859207e-05,
      "loss": 1.2301,
      "step": 2405
    },
    {
      "epoch": 8.700361010830324,
      "grad_norm": 1.9212875366210938,
      "learning_rate": 1.299638989169675e-05,
      "loss": 1.2426,
      "step": 2410
    },
    {
      "epoch": 8.71841155234657,
      "grad_norm": 1.9143081903457642,
      "learning_rate": 1.2815884476534295e-05,
      "loss": 1.2729,
      "step": 2415
    },
    {
      "epoch": 8.736462093862816,
      "grad_norm": 1.9961769580841064,
      "learning_rate": 1.263537906137184e-05,
      "loss": 1.2217,
      "step": 2420
    },
    {
      "epoch": 8.754512635379061,
      "grad_norm": 2.098731517791748,
      "learning_rate": 1.2454873646209387e-05,
      "loss": 1.283,
      "step": 2425
    },
    {
      "epoch": 8.772563176895307,
      "grad_norm": 1.8871407508850098,
      "learning_rate": 1.2274368231046932e-05,
      "loss": 1.23,
      "step": 2430
    },
    {
      "epoch": 8.790613718411553,
      "grad_norm": 2.005661725997925,
      "learning_rate": 1.2093862815884477e-05,
      "loss": 1.2657,
      "step": 2435
    },
    {
      "epoch": 8.808664259927799,
      "grad_norm": 1.979119062423706,
      "learning_rate": 1.1913357400722022e-05,
      "loss": 1.2439,
      "step": 2440
    },
    {
      "epoch": 8.826714801444043,
      "grad_norm": 1.9060126543045044,
      "learning_rate": 1.1732851985559568e-05,
      "loss": 1.2262,
      "step": 2445
    },
    {
      "epoch": 8.844765342960288,
      "grad_norm": 2.2510247230529785,
      "learning_rate": 1.1552346570397113e-05,
      "loss": 1.2121,
      "step": 2450
    },
    {
      "epoch": 8.862815884476534,
      "grad_norm": 1.86028254032135,
      "learning_rate": 1.1371841155234658e-05,
      "loss": 1.2373,
      "step": 2455
    },
    {
      "epoch": 8.88086642599278,
      "grad_norm": 1.9175231456756592,
      "learning_rate": 1.1191335740072201e-05,
      "loss": 1.222,
      "step": 2460
    },
    {
      "epoch": 8.898916967509026,
      "grad_norm": 2.1269476413726807,
      "learning_rate": 1.1010830324909748e-05,
      "loss": 1.2464,
      "step": 2465
    },
    {
      "epoch": 8.916967509025271,
      "grad_norm": 2.002310276031494,
      "learning_rate": 1.0830324909747293e-05,
      "loss": 1.2401,
      "step": 2470
    },
    {
      "epoch": 8.935018050541515,
      "grad_norm": 1.7998039722442627,
      "learning_rate": 1.0649819494584838e-05,
      "loss": 1.2201,
      "step": 2475
    },
    {
      "epoch": 8.953068592057761,
      "grad_norm": 1.8107751607894897,
      "learning_rate": 1.0469314079422383e-05,
      "loss": 1.2578,
      "step": 2480
    },
    {
      "epoch": 8.971119133574007,
      "grad_norm": 1.9096769094467163,
      "learning_rate": 1.028880866425993e-05,
      "loss": 1.2333,
      "step": 2485
    },
    {
      "epoch": 8.989169675090253,
      "grad_norm": 1.9660162925720215,
      "learning_rate": 1.0108303249097473e-05,
      "loss": 1.2348,
      "step": 2490
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.0769301652908325,
      "eval_runtime": 109.3944,
      "eval_samples_per_second": 8.099,
      "eval_steps_per_second": 1.015,
      "step": 2493
    },
    {
      "epoch": 9.007220216606498,
      "grad_norm": 1.9932268857955933,
      "learning_rate": 9.927797833935018e-06,
      "loss": 1.229,
      "step": 2495
    },
    {
      "epoch": 9.025270758122744,
      "grad_norm": 1.8685433864593506,
      "learning_rate": 9.747292418772563e-06,
      "loss": 1.2285,
      "step": 2500
    },
    {
      "epoch": 9.04332129963899,
      "grad_norm": 1.906494379043579,
      "learning_rate": 9.56678700361011e-06,
      "loss": 1.2201,
      "step": 2505
    },
    {
      "epoch": 9.061371841155236,
      "grad_norm": 1.9762567281723022,
      "learning_rate": 9.386281588447654e-06,
      "loss": 1.2743,
      "step": 2510
    },
    {
      "epoch": 9.07942238267148,
      "grad_norm": 2.053436040878296,
      "learning_rate": 9.2057761732852e-06,
      "loss": 1.2278,
      "step": 2515
    },
    {
      "epoch": 9.097472924187725,
      "grad_norm": 1.9371190071105957,
      "learning_rate": 9.025270758122744e-06,
      "loss": 1.2478,
      "step": 2520
    },
    {
      "epoch": 9.115523465703971,
      "grad_norm": 1.8482481241226196,
      "learning_rate": 8.84476534296029e-06,
      "loss": 1.2456,
      "step": 2525
    },
    {
      "epoch": 9.133574007220217,
      "grad_norm": 1.8732579946517944,
      "learning_rate": 8.664259927797834e-06,
      "loss": 1.2096,
      "step": 2530
    },
    {
      "epoch": 9.151624548736462,
      "grad_norm": 1.9462958574295044,
      "learning_rate": 8.483754512635379e-06,
      "loss": 1.2716,
      "step": 2535
    },
    {
      "epoch": 9.169675090252708,
      "grad_norm": 1.9325594902038574,
      "learning_rate": 8.303249097472924e-06,
      "loss": 1.2224,
      "step": 2540
    },
    {
      "epoch": 9.187725631768952,
      "grad_norm": 2.0221385955810547,
      "learning_rate": 8.12274368231047e-06,
      "loss": 1.2644,
      "step": 2545
    },
    {
      "epoch": 9.205776173285198,
      "grad_norm": 2.0052490234375,
      "learning_rate": 7.942238267148016e-06,
      "loss": 1.1987,
      "step": 2550
    },
    {
      "epoch": 9.223826714801444,
      "grad_norm": 2.0276260375976562,
      "learning_rate": 7.76173285198556e-06,
      "loss": 1.2657,
      "step": 2555
    },
    {
      "epoch": 9.24187725631769,
      "grad_norm": 2.114576578140259,
      "learning_rate": 7.581227436823105e-06,
      "loss": 1.2722,
      "step": 2560
    },
    {
      "epoch": 9.259927797833935,
      "grad_norm": 2.0081498622894287,
      "learning_rate": 7.4007220216606496e-06,
      "loss": 1.2157,
      "step": 2565
    },
    {
      "epoch": 9.277978339350181,
      "grad_norm": 2.072906017303467,
      "learning_rate": 7.220216606498195e-06,
      "loss": 1.2119,
      "step": 2570
    },
    {
      "epoch": 9.296028880866427,
      "grad_norm": 2.021022081375122,
      "learning_rate": 7.03971119133574e-06,
      "loss": 1.1928,
      "step": 2575
    },
    {
      "epoch": 9.314079422382672,
      "grad_norm": 2.041470766067505,
      "learning_rate": 6.859205776173286e-06,
      "loss": 1.2328,
      "step": 2580
    },
    {
      "epoch": 9.332129963898916,
      "grad_norm": 1.8146048784255981,
      "learning_rate": 6.678700361010831e-06,
      "loss": 1.2198,
      "step": 2585
    },
    {
      "epoch": 9.350180505415162,
      "grad_norm": 1.8601428270339966,
      "learning_rate": 6.498194945848375e-06,
      "loss": 1.2192,
      "step": 2590
    },
    {
      "epoch": 9.368231046931408,
      "grad_norm": 2.1687159538269043,
      "learning_rate": 6.31768953068592e-06,
      "loss": 1.2128,
      "step": 2595
    },
    {
      "epoch": 9.386281588447654,
      "grad_norm": 2.2517902851104736,
      "learning_rate": 6.137184115523466e-06,
      "loss": 1.2583,
      "step": 2600
    },
    {
      "epoch": 9.4043321299639,
      "grad_norm": 2.1035969257354736,
      "learning_rate": 5.956678700361011e-06,
      "loss": 1.2223,
      "step": 2605
    },
    {
      "epoch": 9.422382671480145,
      "grad_norm": 1.991341471672058,
      "learning_rate": 5.776173285198557e-06,
      "loss": 1.2848,
      "step": 2610
    },
    {
      "epoch": 9.440433212996389,
      "grad_norm": 1.926889181137085,
      "learning_rate": 5.595667870036101e-06,
      "loss": 1.2374,
      "step": 2615
    },
    {
      "epoch": 9.458483754512635,
      "grad_norm": 2.0848920345306396,
      "learning_rate": 5.4151624548736465e-06,
      "loss": 1.266,
      "step": 2620
    },
    {
      "epoch": 9.47653429602888,
      "grad_norm": 1.9739658832550049,
      "learning_rate": 5.2346570397111915e-06,
      "loss": 1.2509,
      "step": 2625
    },
    {
      "epoch": 9.494584837545126,
      "grad_norm": 1.9265848398208618,
      "learning_rate": 5.054151624548736e-06,
      "loss": 1.2117,
      "step": 2630
    },
    {
      "epoch": 9.512635379061372,
      "grad_norm": 2.124898672103882,
      "learning_rate": 4.873646209386281e-06,
      "loss": 1.2672,
      "step": 2635
    },
    {
      "epoch": 9.530685920577618,
      "grad_norm": 2.058809757232666,
      "learning_rate": 4.693140794223827e-06,
      "loss": 1.1857,
      "step": 2640
    },
    {
      "epoch": 9.548736462093864,
      "grad_norm": 2.029448986053467,
      "learning_rate": 4.512635379061372e-06,
      "loss": 1.2063,
      "step": 2645
    },
    {
      "epoch": 9.566787003610107,
      "grad_norm": 2.223456859588623,
      "learning_rate": 4.332129963898917e-06,
      "loss": 1.2311,
      "step": 2650
    },
    {
      "epoch": 9.584837545126353,
      "grad_norm": 1.9243791103363037,
      "learning_rate": 4.151624548736462e-06,
      "loss": 1.268,
      "step": 2655
    },
    {
      "epoch": 9.602888086642599,
      "grad_norm": 2.2260873317718506,
      "learning_rate": 3.971119133574008e-06,
      "loss": 1.2613,
      "step": 2660
    },
    {
      "epoch": 9.620938628158845,
      "grad_norm": 2.1329386234283447,
      "learning_rate": 3.7906137184115523e-06,
      "loss": 1.2297,
      "step": 2665
    },
    {
      "epoch": 9.63898916967509,
      "grad_norm": 1.965458869934082,
      "learning_rate": 3.6101083032490977e-06,
      "loss": 1.2138,
      "step": 2670
    },
    {
      "epoch": 9.657039711191336,
      "grad_norm": 1.978442668914795,
      "learning_rate": 3.429602888086643e-06,
      "loss": 1.2037,
      "step": 2675
    },
    {
      "epoch": 9.675090252707582,
      "grad_norm": 1.8846327066421509,
      "learning_rate": 3.2490974729241876e-06,
      "loss": 1.2415,
      "step": 2680
    },
    {
      "epoch": 9.693140794223826,
      "grad_norm": 2.1255033016204834,
      "learning_rate": 3.068592057761733e-06,
      "loss": 1.2564,
      "step": 2685
    },
    {
      "epoch": 9.711191335740072,
      "grad_norm": 2.009032964706421,
      "learning_rate": 2.8880866425992783e-06,
      "loss": 1.2276,
      "step": 2690
    },
    {
      "epoch": 9.729241877256317,
      "grad_norm": 1.8915256261825562,
      "learning_rate": 2.7075812274368233e-06,
      "loss": 1.2204,
      "step": 2695
    },
    {
      "epoch": 9.747292418772563,
      "grad_norm": 2.1508054733276367,
      "learning_rate": 2.527075812274368e-06,
      "loss": 1.2347,
      "step": 2700
    },
    {
      "epoch": 9.765342960288809,
      "grad_norm": 1.8396527767181396,
      "learning_rate": 2.3465703971119136e-06,
      "loss": 1.2605,
      "step": 2705
    },
    {
      "epoch": 9.783393501805055,
      "grad_norm": 1.8777799606323242,
      "learning_rate": 2.1660649819494585e-06,
      "loss": 1.2537,
      "step": 2710
    },
    {
      "epoch": 9.8014440433213,
      "grad_norm": 2.144824266433716,
      "learning_rate": 1.985559566787004e-06,
      "loss": 1.2456,
      "step": 2715
    },
    {
      "epoch": 9.819494584837544,
      "grad_norm": 2.033395528793335,
      "learning_rate": 1.8050541516245488e-06,
      "loss": 1.2164,
      "step": 2720
    },
    {
      "epoch": 9.83754512635379,
      "grad_norm": 1.9796704053878784,
      "learning_rate": 1.6245487364620938e-06,
      "loss": 1.2292,
      "step": 2725
    },
    {
      "epoch": 9.855595667870036,
      "grad_norm": 1.9123731851577759,
      "learning_rate": 1.4440433212996392e-06,
      "loss": 1.244,
      "step": 2730
    },
    {
      "epoch": 9.873646209386282,
      "grad_norm": 1.9208468198776245,
      "learning_rate": 1.263537906137184e-06,
      "loss": 1.284,
      "step": 2735
    },
    {
      "epoch": 9.891696750902527,
      "grad_norm": 1.9225188493728638,
      "learning_rate": 1.0830324909747293e-06,
      "loss": 1.2003,
      "step": 2740
    },
    {
      "epoch": 9.909747292418773,
      "grad_norm": 2.104917049407959,
      "learning_rate": 9.025270758122744e-07,
      "loss": 1.2411,
      "step": 2745
    },
    {
      "epoch": 9.927797833935019,
      "grad_norm": 1.9824825525283813,
      "learning_rate": 7.220216606498196e-07,
      "loss": 1.2332,
      "step": 2750
    },
    {
      "epoch": 9.945848375451263,
      "grad_norm": 1.996358036994934,
      "learning_rate": 5.415162454873646e-07,
      "loss": 1.2442,
      "step": 2755
    },
    {
      "epoch": 9.963898916967509,
      "grad_norm": 2.0462915897369385,
      "learning_rate": 3.610108303249098e-07,
      "loss": 1.2396,
      "step": 2760
    },
    {
      "epoch": 9.981949458483754,
      "grad_norm": 2.0714359283447266,
      "learning_rate": 1.805054151624549e-07,
      "loss": 1.1794,
      "step": 2765
    },
    {
      "epoch": 10.0,
      "grad_norm": 1.9529480934143066,
      "learning_rate": 0.0,
      "loss": 1.243,
      "step": 2770
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.0758564472198486,
      "eval_runtime": 109.5734,
      "eval_samples_per_second": 8.086,
      "eval_steps_per_second": 1.013,
      "step": 2770
    }
  ],
  "logging_steps": 5,
  "max_steps": 2770,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.669950411309056e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
